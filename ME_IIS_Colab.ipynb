{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be8a489",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kaminglui/Domain-Adaptation-with-ME-IIS/blob/main/ME_IIS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c57dfc",
   "metadata": {
    "id": "f3c57dfc"
   },
   "source": [
    "# ME-IIS Domain Adaptation (Colab)\n",
    "Single-pass pipeline to train a source ResNet-50 and adapt with ME-IIS. Edit the Section 3 config cell to switch datasets, domains, and options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffbda6",
   "metadata": {
    "id": "71ffbda6"
   },
   "source": [
    "## 0. Setup & GPU check\n",
    "- Mount Google Drive and choose a working directory.\n",
    "- Clone/pull the repo into that folder.\n",
    "- Confirm CUDA availability and show the GPU name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e1d3bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99e1d3bf",
    "outputId": "7cff1547-b238-44d9-9107-cd5ca3d9593c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "WORK_DIR: /content/drive/MyDrive/MEIIS-Colab\n",
      "[Repo] Repository exists; pulling latest changes...\n",
      "remote: Enumerating objects: 62, done.\u001b[K\n",
      "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 48 (delta 31), reused 32 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (48/48), 51.75 KiB | 13.00 KiB/s, done.\n",
      "From https://github.com/kaminglui/Domain-Adaptation-with-ME-IIS\n",
      "   3459174..761a61e  main       -> origin/main\n",
      "Updating 3459174..761a61e\n",
      "Fast-forward\n",
      " ME_IIS_Colab.ipynb                | 559 \u001b[32m++++++++++++++++++++++++++++++++++++++\u001b[m\n",
      " README.md                         |  13 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " datasets/domain_loaders.py        |  68 \u001b[32m++++\u001b[m\u001b[31m-\u001b[m\n",
      " models/me_iis_adapter.py          | 100 \u001b[32m++++++\u001b[m\u001b[31m-\u001b[m\n",
      " run_smoke_tests.py                | 274 \u001b[32m+++++++++++++\u001b[m\u001b[31m------\u001b[m\n",
      " scripts/adapt_me_iis.py           | 102 \u001b[32m++\u001b[m\u001b[31m-----\u001b[m\n",
      " scripts/demo_me_iis_toy.py        |  71 \u001b[32m+++++\u001b[m\n",
      " scripts/plot_iis_dynamics.py      |  23 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " scripts/run_me_iis_experiments.py | 516 \u001b[32m+++++++++++++++++++++++++++++++++++\u001b[m\n",
      " scripts/train_source.py           |  37 \u001b[32m+\u001b[m\u001b[31m--\u001b[m\n",
      " tests/test_experiment_utils.py    |  84 \u001b[32m++++++\u001b[m\n",
      " tests/test_me_iis_additional.py   | 184 \u001b[32m++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
      " utils/experiment_utils.py         | 120 \u001b[32m++++++++\u001b[m\n",
      " utils/feature_utils.py            |  49 \u001b[32m++++\u001b[m\n",
      " 14 files changed, 1994 insertions(+), 206 deletions(-)\n",
      " create mode 100644 ME_IIS_Colab.ipynb\n",
      " create mode 100644 scripts/demo_me_iis_toy.py\n",
      " create mode 100644 scripts/run_me_iis_experiments.py\n",
      " create mode 100644 tests/test_experiment_utils.py\n",
      " create mode 100644 utils/experiment_utils.py\n",
      " create mode 100644 utils/feature_utils.py\n",
      "[Repo] Current repo dir: /content/drive/MyDrive/MEIIS-Colab/Domain-Adaptation-with-ME-IIS\n",
      "torch.cuda.is_available(): True\n",
      "GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# Section 0 - Setup & GPU check\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "USE_DRIVE = True  # Set False to keep everything in /content\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive\" if USE_DRIVE else \"/content\"\n",
    "WORK_DIR = os.path.join(DRIVE_ROOT, \"MEIIS-Colab\")\n",
    "REPO_URL = \"https://github.com/kaminglui/Domain-Adaptation-with-ME-IIS.git\"\n",
    "REPO_NAME = \"Domain-Adaptation-with-ME-IIS\"\n",
    "REPO_DIR = os.path.join(WORK_DIR, REPO_NAME)\n",
    "\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(\"WORK_DIR:\", WORK_DIR)\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    print(\"[Repo] Cloning repository...\")\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(\"[Repo] Repository exists; pulling latest changes...\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    !git pull\n",
    "    os.chdir(WORK_DIR)\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"[Repo] Current repo dir:\", os.getcwd())\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"GPU not detected - switch runtime to GPU for full runs.\")\n",
    "except ImportError as exc:\n",
    "    print(\"PyTorch not installed yet; run Section 1 to install dependencies. GPU check skipped:\", exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125ea71",
   "metadata": {
    "id": "3125ea71"
   },
   "source": [
    "## 1. Install dependencies\n",
    "Re-run this cell if the Colab runtime restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae70533",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ae70533",
    "outputId": "c83ea5b8-7c47-467a-a4ed-48aa88338033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies from: env/requirements_colab.txt\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 2)) (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 5)) (3.10.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 7)) (2.19.0)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from -r env/requirements_colab.txt (line 8)) (0.3.13)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r env/requirements_colab.txt (line 2)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r env/requirements_colab.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r env/requirements_colab.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r env/requirements_colab.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r env/requirements_colab.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r env/requirements_colab.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (5.29.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r env/requirements_colab.txt (line 7)) (3.1.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r env/requirements_colab.txt (line 8)) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->-r env/requirements_colab.txt (line 8)) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->-r env/requirements_colab.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r env/requirements_colab.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r env/requirements_colab.txt (line 8)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r env/requirements_colab.txt (line 8)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r env/requirements_colab.txt (line 8)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->-r env/requirements_colab.txt (line 8)) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# Section 1 - Install dependencies (re-run if the runtime restarts)\n",
    "import os\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "req_file = \"env/requirements_colab.txt\" if os.path.exists(\"env/requirements_colab.txt\") else \"env/requirements.txt\"\n",
    "print(\"Installing dependencies from:\", req_file)\n",
    "!pip install -r {req_file}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45725f",
   "metadata": {
    "id": "4f45725f"
   },
   "source": [
    "## 2. Download datasets via KaggleHub\n",
    "Downloads Office-Home and Office-31 with KaggleHub, locates the canonical roots, and links them under `datasets/` for the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7b9957",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd7b9957",
    "outputId": "a571e358-b67b-438d-b2aa-650a34608934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo dir: /content/drive/MyDrive/MEIIS-Colab/Domain-Adaptation-with-ME-IIS\n",
      "[Data] Downloading Office-Home (lhrrraname/officehome)...\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/lhrrraname/officehome?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.06G/1.06G [00:13<00:00, 84.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Office-Home root: /root/.cache/kagglehub/datasets/lhrrraname/officehome/versions/1/datasets/OfficeHomeDataset_10072016\n",
      "[Data] Downloading Office-31 (xixuhu/office31)...\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/xixuhu/office31?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75.9M/75.9M [00:00<00:00, 93.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Office-31 root: /root/.cache/kagglehub/datasets/xixuhu/office31/versions/1/Office-31\n",
      "[Data] Linked datasets/Office-Home -> /root/.cache/kagglehub/datasets/lhrrraname/officehome/versions/1/datasets/OfficeHomeDataset_10072016\n",
      "[Data] Linked datasets/Office-31 -> /root/.cache/kagglehub/datasets/xixuhu/office31/versions/1/Office-31\n",
      "[Data] Office-Home DATA_ROOT: datasets/Office-Home\n",
      "[Data] Office-31 DATA_ROOT: datasets/Office-31\n"
     ]
    }
   ],
   "source": [
    "# Section 2 - Download datasets via KaggleHub\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"Repo dir:\", os.getcwd())\n",
    "\n",
    "try:\n",
    "    import kagglehub  # type: ignore\n",
    "except ImportError:\n",
    "    print(\"[Data] Installing kagglehub...\")\n",
    "    !pip install kagglehub\n",
    "    import kagglehub  # type: ignore\n",
    "\n",
    "def _find_office_home_root(base_dir: pathlib.Path) -> pathlib.Path:\n",
    "    candidates = [base_dir] + [p for p in base_dir.rglob(\"*\") if p.is_dir()]\n",
    "    for cand in candidates:\n",
    "        names = {p.name for p in cand.iterdir() if p.is_dir()}\n",
    "        if {\"Art\", \"Clipart\", \"Product\"} <= names and any(n.lower().startswith(\"real\") for n in names):\n",
    "            return cand\n",
    "    return base_dir\n",
    "\n",
    "def _find_office31_root(base_dir: pathlib.Path) -> pathlib.Path:\n",
    "    candidates = [base_dir] + [p for p in base_dir.rglob(\"*\") if p.is_dir()]\n",
    "    for cand in candidates:\n",
    "        names = {p.name.lower() for p in cand.iterdir() if p.is_dir()}\n",
    "        if {\"amazon\", \"dslr\", \"webcam\"} <= names:\n",
    "            return cand\n",
    "    return base_dir\n",
    "\n",
    "print(\"[Data] Downloading Office-Home (lhrrraname/officehome)...\")\n",
    "office_home_raw = pathlib.Path(kagglehub.dataset_download(\"lhrrraname/officehome\"))\n",
    "office_home_root = _find_office_home_root(office_home_raw)\n",
    "print(\"  Office-Home root:\", office_home_root)\n",
    "\n",
    "print(\"[Data] Downloading Office-31 (xixuhu/office31)...\")\n",
    "office31_raw = pathlib.Path(kagglehub.dataset_download(\"xixuhu/office31\"))\n",
    "office31_root = _find_office31_root(office31_raw)\n",
    "print(\"  Office-31 root:\", office31_root)\n",
    "\n",
    "datasets_dir = pathlib.Path(\"datasets\")\n",
    "datasets_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def _ensure_link(link_path: pathlib.Path, target: pathlib.Path) -> None:\n",
    "    target = target.resolve()\n",
    "    if link_path.exists() and not link_path.is_symlink():\n",
    "        print(f\"[Data] {link_path} exists and is not a symlink; leaving as-is.\")\n",
    "        return\n",
    "    if link_path.is_symlink():\n",
    "        current = link_path.resolve()\n",
    "        if current == target:\n",
    "            print(f\"[Data] {link_path} already points to {target}\")\n",
    "            return\n",
    "        link_path.unlink()\n",
    "    try:\n",
    "        link_path.symlink_to(target, target_is_directory=True)\n",
    "        print(f\"[Data] Linked {link_path} -> {target}\")\n",
    "    except OSError as exc:\n",
    "        print(f\"[Data] Could not create symlink {link_path} -> {target}: {exc}\")\n",
    "\n",
    "_ensure_link(datasets_dir / \"Office-Home\", office_home_root)\n",
    "_ensure_link(datasets_dir / \"Office-31\", office31_root)\n",
    "\n",
    "OFFICE_HOME_ROOT = datasets_dir / \"Office-Home\"\n",
    "OFFICE31_ROOT = datasets_dir / \"Office-31\"\n",
    "print(\"[Data] Office-Home DATA_ROOT:\", OFFICE_HOME_ROOT)\n",
    "print(\"[Data] Office-31 DATA_ROOT:\", OFFICE31_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff21e55",
   "metadata": {
    "id": "7ff21e55"
   },
   "source": [
    "## 3. Configuration (single cell with all knobs)\n",
    "Edit only in this cell to change experiment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361d1911",
   "metadata": {
    "id": "361d1911"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Edit only in this cell to change experiment settings.\n",
    "\n",
    "# Dataset & domains\n",
    "DATASET_NAME = \"office_home\"  # or \"office31\"\n",
    "SOURCE_DOMAIN = \"Ar\"          # e.g. Ar/Cl/Pr/Rw for Office-Home, A/D/W for Office-31\n",
    "TARGET_DOMAIN = \"Cl\"\n",
    "SEED = 0\n",
    "\n",
    "# Paths (assume we are in the repo root)\n",
    "DATA_ROOT = \"datasets/Office-Home\"  # or \"datasets/Office-31\"\n",
    "\n",
    "# Source training hyperparameters\n",
    "NUM_EPOCHS_SRC = 100\n",
    "BATCH_SIZE_SRC = 32\n",
    "LR_BACKBONE = 1e-3\n",
    "LR_CLASSIFIER = 1e-2\n",
    "WEIGHT_DECAY = 1e-3\n",
    "NUM_WORKERS = 4\n",
    "DETERMINISTIC = True  # set True to minimize randomness\n",
    "\n",
    "# ME-IIS / adaptation hyperparameters\n",
    "# We found that using layer3+layer4 preserves spatial/style information.\n",
    "# Including avgpool (global pooled features) sharply reduced performance,\n",
    "# so we disable it by default. You can add it back by changing this string.\n",
    "FEATURE_LAYERS = \"layer3,layer4\"\n",
    "GMM_SELECTION_MODE = \"bic\"        # \"fixed\" or \"bic\"\n",
    "NUM_LATENT_STYLES = 5             # default components per layer (fixed mode or BIC init)\n",
    "GMM_BIC_MIN_COMPONENTS = 2        # BIC lower bound\n",
    "GMM_BIC_MAX_COMPONENTS = 8        # BIC upper bound\n",
    "\n",
    "# NEW: source_prob_mode\n",
    "SOURCE_PROB_MODE = \"softmax\"      # or \"onehot\" (use GT labels for source constraints)\n",
    "\n",
    "# NEW: optional per-layer GMM override\n",
    "# e.g. \"layer3:10,layer4:5\" or \"7,8,9\" for 3 layers; leave \"\" to disable\n",
    "COMPONENTS_OVERRIDE = \"\"\n",
    "\n",
    "# Pseudo-label adaptation (ME-IIS+PL)\n",
    "USE_PSEUDO_LABELS = False         # base flag for direct runs\n",
    "PSEUDO_CONF_THRESH = 0.9\n",
    "PSEUDO_MAX_RATIO = 1.0            # 1.0 = no cap; or e.g. 0.3 to limit pseudo-target count\n",
    "PSEUDO_LOSS_WEIGHT = 1.0\n",
    "\n",
    "# IIS/Adapt config\n",
    "IIS_ITERS = 15\n",
    "IIS_TOL = 1e-3\n",
    "ADAPT_EPOCHS = 10\n",
    "FINETUNE_BACKBONE = False\n",
    "BACKBONE_LR_SCALE = 0.1\n",
    "CLASSIFIER_LR = 1e-2\n",
    "ADAPT_WEIGHT_DECAY = 1e-3\n",
    "\n",
    "# NEW: warm start for pseudo-labels\n",
    "WARMUP_EPOCHS_NO_PL = 5   # Stage-1 epochs with no pseudo-labels\n",
    "STAGE2_EPOCHS_PL = 10     # Stage-2 epochs with pseudo-labels\n",
    "\n",
    "# Optional: force fresh runs instead of auto-resume\n",
    "FORCE_FRESH_SOURCE_TRAIN = False\n",
    "FORCE_FRESH_ADAPT = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d56b10e",
   "metadata": {
    "id": "3d56b10e"
   },
   "source": [
    "## 4. Helper: construct checkpoint paths and cleanup\n",
    "Lets you intentionally bypass auto-resume by deleting existing checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dffbde8",
   "metadata": {
    "id": "1dffbde8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "def get_source_ckpt_path():\n",
    "    return f\"checkpoints/source_only_{SOURCE_DOMAIN}_to_{TARGET_DOMAIN}_seed{SEED}.pth\"\n",
    "\n",
    "def get_me_iis_ckpt_path():\n",
    "    layers_str = FEATURE_LAYERS.replace(\",\", \"-\").replace(\" \", \"\")\n",
    "    return f\"checkpoints/me_iis_{SOURCE_DOMAIN}_to_{TARGET_DOMAIN}_{layers_str}_seed{SEED}.pth\"\n",
    "\n",
    "if FORCE_FRESH_SOURCE_TRAIN:\n",
    "    src_ckpt = get_source_ckpt_path()\n",
    "    if os.path.exists(src_ckpt):\n",
    "        os.remove(src_ckpt)\n",
    "        print(\"Deleted existing source checkpoint:\", src_ckpt)\n",
    "\n",
    "if FORCE_FRESH_ADAPT:\n",
    "    adapt_ckpt = get_me_iis_ckpt_path()\n",
    "    if os.path.exists(adapt_ckpt):\n",
    "        os.remove(adapt_ckpt)\n",
    "        print(\"Deleted existing ME-IIS checkpoint:\", adapt_ckpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7407a",
   "metadata": {
    "id": "09d7407a"
   },
   "source": [
    "## 5. Train source-only model (full training)\n",
    "Build the command from the config above and stream output directly in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b30bd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3b30bd1",
    "outputId": "caeaf7ce-bccc-4e0d-c4ec-def18a1c37f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training command:\n",
      " python scripts/train_source.py   --dataset_name office_home   --data_root \"datasets/Office-Home\"   --source_domain Ar   --target_domain Cl   --num_epochs 100   --batch_size 32   --lr_backbone 0.001   --lr_classifier 0.01   --weight_decay 0.001   --num_workers 4   --deterministic   --seed 0\n",
      "2025-12-10 04:11:20.682777: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 04:11:20.699436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765339880.720313    2704 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765339880.726766    2704 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765339880.742995    2704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765339880.743020    2704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765339880.743022    2704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765339880.743025    2704 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-10 04:11:20.747812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[CKPT] Auto-resume: found existing source checkpoint at checkpoints/source_only_Ar_to_Cl_seed0.pth\n",
      "[Seed] Using seed 0 (deterministic=True)\n",
      "[DEVICE] Using cuda (GPU) 0\n",
      "[DEBUG] Built loaders: len(source_loader)=76, len(target_eval_loader)=137\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100% 97.8M/97.8M [00:00<00:00, 231MB/s]\n",
      "[CKPT] Resuming from checkpoint: checkpoints/source_only_Ar_to_Cl_seed0.pth (start_epoch=101, best_target_acc=93.75)\n",
      "[TRAIN] Starting source-only training: dataset=office_home, source=Ar, target=Cl, dry_run_max_batches=0\n",
      "Epoch: 0it [00:00, ?it/s]\n",
      "[TRAIN] Finished training loop, preparing checkpoint...\n",
      "[CKPT] Saving checkpoint to: checkpoints/source_only_Ar_to_Cl_seed0.pth\n",
      "[CKPT] Done saving checkpoint: checkpoints/source_only_Ar_to_Cl_seed0.pth\n",
      "[TENSORBOARD] Logs written to runs/source_only\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "det_flag = \"--deterministic\" if DETERMINISTIC else \"\"\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "SRC_CMD = (\n",
    "    'python scripts/train_source.py \\\n",
    "'\n",
    "    f'  --dataset_name {DATASET_NAME} \\\n",
    "'\n",
    "    f'  --data_root \"{DATA_ROOT}\" \\\n",
    "'\n",
    "    f'  --source_domain {SOURCE_DOMAIN} \\\n",
    "'\n",
    "    f'  --target_domain {TARGET_DOMAIN} \\\n",
    "'\n",
    "    f'  --num_epochs {NUM_EPOCHS_SRC} \\\n",
    "'\n",
    "    f'  --batch_size {BATCH_SIZE_SRC} \\\n",
    "'\n",
    "    f'  --lr_backbone {LR_BACKBONE} \\\n",
    "'\n",
    "    f'  --lr_classifier {LR_CLASSIFIER} \\\n",
    "'\n",
    "    f'  --weight_decay {WEIGHT_DECAY} \\\n",
    "'\n",
    "    f'  --num_workers {NUM_WORKERS} \\\n",
    "'\n",
    "    f'  {det_flag} \\\n",
    "'\n",
    "    f'  --seed {SEED}'\n",
    ").strip()\n",
    "\n",
    "print(\"Training command:\\n\", SRC_CMD)\n",
    "!{SRC_CMD}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d371b",
   "metadata": {
    "id": "dc0d371b"
   },
   "source": [
    "## 6. Run ME-IIS adaptation\n",
    "Uses the source checkpoint above plus the ME-IIS settings in the config cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccabb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70ccabb8",
    "outputId": "378e3697-bb54-4c40-b2fc-c53ee6fbdc69"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "def build_adapt_cmd(use_pseudo: bool, adapt_epochs: int, extra_tag: str = \"\"):\n",
    "    src_ckpt = get_source_ckpt_path()\n",
    "    det_flag = \"--deterministic\" if DETERMINISTIC else \"\"\n",
    "    cmd = f\"\"\"\n",
    "    python scripts/adapt_me_iis.py       --dataset_name {DATASET_NAME}       --data_root \"{DATA_ROOT}\"       --source_domain {SOURCE_DOMAIN}       --target_domain {TARGET_DOMAIN}       --checkpoint {src_ckpt}       --batch_size {BATCH_SIZE_SRC}       --num_workers {NUM_WORKERS}       --feature_layers \"{FEATURE_LAYERS}\"       --num_latent_styles {NUM_LATENT_STYLES}       --gmm_selection_mode {GMM_SELECTION_MODE}       --gmm_bic_min_components {GMM_BIC_MIN_COMPONENTS}       --gmm_bic_max_components {GMM_BIC_MAX_COMPONENTS}       --iis_iters {IIS_ITERS}       --iis_tol {IIS_TOL}       --adapt_epochs {adapt_epochs}       {'--finetune_backbone' if FINETUNE_BACKBONE else ''}       --backbone_lr_scale {BACKBONE_LR_SCALE}       --classifier_lr {CLASSIFIER_LR}       --weight_decay {ADAPT_WEIGHT_DECAY}       --source_prob_mode {SOURCE_PROB_MODE}       --seed {SEED}       {det_flag}\n",
    "    \"\"\"\n",
    "    if COMPONENTS_OVERRIDE.strip():\n",
    "        cmd += f' --components_per_layer \"{COMPONENTS_OVERRIDE.strip()}\" '\n",
    "\n",
    "    if use_pseudo:\n",
    "        cmd += (\n",
    "            f\" --use_pseudo_labels --pseudo_conf_thresh {PSEUDO_CONF_THRESH} \"\n",
    "            f\"--pseudo_max_ratio {PSEUDO_MAX_RATIO} --pseudo_loss_weight {PSEUDO_LOSS_WEIGHT}\"\n",
    "        )\n",
    "\n",
    "    return cmd.strip()\n",
    "\n",
    "ran_stage = False\n",
    "\n",
    "# Stage 1: warm-up ME-IIS without pseudo-labels\n",
    "if WARMUP_EPOCHS_NO_PL > 0:\n",
    "    ran_stage = True\n",
    "    print(f\"Running ME-IIS warm-up without pseudo-labels for {WARMUP_EPOCHS_NO_PL} epochs...\")\n",
    "    ADAPT_CMD_STAGE1 = build_adapt_cmd(use_pseudo=False, adapt_epochs=WARMUP_EPOCHS_NO_PL, extra_tag=\"_stage1\")\n",
    "    print(\"Stage-1 command:\n",
    "\" + ADAPT_CMD_STAGE1)\n",
    "    print(\"Stage-1 starts from the source-only checkpoint and writes the standard ME-IIS checkpoint.\")\n",
    "    !{ADAPT_CMD_STAGE1}\n",
    "\n",
    "# Stage 2: ME-IIS with pseudo-labels\n",
    "if STAGE2_EPOCHS_PL > 0:\n",
    "    ran_stage = True\n",
    "    print(f\"Running ME-IIS with pseudo-labels for {STAGE2_EPOCHS_PL} epochs...\")\n",
    "    ADAPT_CMD_STAGE2 = build_adapt_cmd(use_pseudo=True, adapt_epochs=STAGE2_EPOCHS_PL, extra_tag=\"_stage2\")\n",
    "    print(\"Stage-2 command:\n",
    "\" + ADAPT_CMD_STAGE2)\n",
    "    print(\n",
    "        \"Stage-2 also points to the source-only checkpoint; adapt_me_iis.py will auto-resume from \"\n",
    "        \"any existing ME-IIS checkpoint (e.g., the Stage-1 weights) and overwrite it with the pseudo-label run.\"\n",
    "    )\n",
    "    !{ADAPT_CMD_STAGE2}\n",
    "\n",
    "# Fallback: single-stage adaptation (original behavior)\n",
    "if not ran_stage:\n",
    "    print(\"Running single-stage adaptation (no warm start configured)...\")\n",
    "    ADAPT_CMD = build_adapt_cmd(use_pseudo=USE_PSEUDO_LABELS, adapt_epochs=ADAPT_EPOCHS)\n",
    "    print(\"Adaptation command:\n",
    "\" + ADAPT_CMD)\n",
    "    !{ADAPT_CMD}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ccf29",
   "metadata": {
    "id": "a40ccf29"
   },
   "source": [
    "## 7. Optional: experiment driver examples\n",
    "Leave `RUN_EXPERIMENT_DRIVER = False` for the standard train + adapt path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4236ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec4236ee",
    "outputId": "e6d9066d-dc07-491c-b324-7228dd666b09"
   },
   "outputs": [],
   "source": [
    "RUN_EXPERIMENT_DRIVER = True\n",
    "\n",
    "if RUN_EXPERIMENT_DRIVER:\n",
    "    import os\n",
    "    os.chdir(REPO_DIR)\n",
    "    det_flag = \"--deterministic\" if DETERMINISTIC else \"\"\n",
    "    EXP_CMD = (\n",
    "        'python scripts/run_me_iis_experiments.py \\\n",
    "'\n",
    "        f'  --dataset_name {DATASET_NAME} \\\n",
    "'\n",
    "        f'  --source_domain {SOURCE_DOMAIN} \\\n",
    "'\n",
    "        f'  --target_domain {TARGET_DOMAIN} \\\n",
    "'\n",
    "        '  --experiment_family layers \\\n",
    "'\n",
    "        f'  --seeds {SEED} \\\n",
    "'\n",
    "        f'  --base_data_root \"{DATA_ROOT}\" \\\n",
    "'\n",
    "        f'  --feature_layers \"{FEATURE_LAYERS}\" \\\n",
    "'\n",
    "        f'  --num_latent_styles {NUM_LATENT_STYLES} \\\n",
    "'\n",
    "        f'  --gmm_selection_mode {GMM_SELECTION_MODE} \\\n",
    "'\n",
    "        f'  --gmm_bic_min_components {GMM_BIC_MIN_COMPONENTS} \\\n",
    "'\n",
    "        f'  --gmm_bic_max_components {GMM_BIC_MAX_COMPONENTS} \\\n",
    "'\n",
    "        f'  --source_prob_mode {SOURCE_PROB_MODE} \\\n",
    "'\n",
    "        f'  --num_epochs {NUM_EPOCHS_SRC} \\\n",
    "'\n",
    "        f'  --batch_size {BATCH_SIZE_SRC} \\\n",
    "'\n",
    "        f'  --num_workers {NUM_WORKERS} \\\n",
    "'\n",
    "        f'  --iis_iters {IIS_ITERS} \\\n",
    "'\n",
    "        f'  --iis_tol {IIS_TOL} \\\n",
    "'\n",
    "        f'  --adapt_epochs {ADAPT_EPOCHS} \\\n",
    "'\n",
    "        f'  --backbone_lr_scale {BACKBONE_LR_SCALE} \\\n",
    "'\n",
    "        f'  --classifier_lr {CLASSIFIER_LR} \\\n",
    "'\n",
    "        f'  --weight_decay {ADAPT_WEIGHT_DECAY} \\\n",
    "'\n",
    "        f'  {'--finetune_backbone' if FINETUNE_BACKBONE else ''} \\\n",
    "'\n",
    "        f'  {det_flag} \\\n",
    "'\n",
    "        f'  --output_csv results/me_iis_layers_{SOURCE_DOMAIN}_to_{TARGET_DOMAIN}_seed{SEED}.csv'\n",
    "    ).strip()\n",
    "\n",
    "    if COMPONENTS_OVERRIDE.strip():\n",
    "        EXP_CMD += f' --components_per_layer \"{COMPONENTS_OVERRIDE}\"'\n",
    "\n",
    "    if USE_PSEUDO_LABELS:\n",
    "        EXP_CMD += (\n",
    "            f\" --pseudo_conf_thresh {PSEUDO_CONF_THRESH} \"\n",
    "            f\"--pseudo_max_ratio {PSEUDO_MAX_RATIO} --pseudo_loss_weight {PSEUDO_LOSS_WEIGHT}\"\n",
    "        )\n",
    "\n",
    "    if det_flag:\n",
    "        EXP_CMD += f\" {det_flag}\"\n",
    "\n",
    "    print(\"Experiment driver command:\\n\", EXP_CMD)\n",
    "    !{EXP_CMD}\n",
    "else:\n",
    "    print(\"Experiment driver is disabled (set RUN_EXPERIMENT_DRIVER = True to run).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb1c47",
   "metadata": {
    "id": "c1eb1c47"
   },
   "source": [
    "## 8. Notes on outputs\n",
    "- Source checkpoints: `checkpoints/source_only_*` (auto-resume uses the matching file name).\n",
    "- ME-IIS checkpoints and IIS weights: `checkpoints/me_iis_*` and `results/me_iis_weights_*.npz`.\n",
    "- CSV log of source/adapt runs: `results/office_home_me_iis.csv` (dataset column distinguishes Office-Home vs Office-31).\n",
    "- Experiment driver summaries (optional): `results/me_iis_experiments_summary.csv` or the path you pass via `--output_csv`.\n",
    "- TensorBoard logs live under `runs/` (`runs/source_only` and `runs/adapt_me_iis`)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
