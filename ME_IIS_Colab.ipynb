{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87247027",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kaminglui/Domain-Adaptation-with-ME-IIS/blob/main/ME_IIS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# ME-IIS Domain Adaptation (Colab)\n",
    "\n",
    "Unified, linear pipeline to train a source ResNet-50 and adapt with ME-IIS.\n",
    "\n",
    "- Methods: source-only baseline, ME-IIS (GMM backend), ME-IIS (vMF-softmax backend), optional pseudo-label Stage-2 when enabled.\n",
    "- Protocol: labeled source domain -> ME-IIS adaptation on unlabeled target -> target labels used **only** for evaluation; report target accuracy (%) and mean->std across seeds.\n",
    "- Backends & ablations: compare GMM vs vMF-softmax, optional layer sets [\"layer4\"] vs [\"layer3\",\"layer4\"], optional vMF sweeps over K/kappa/clean_ratio.\n",
    "- Outputs: per-run config JSON, `results/office_home_me_iis.csv`, checkpoints, IIS history `.npz`, TensorBoard logs; all under `output_root/runs/<tag>/`.\n",
    "- Runtime: QUICK_MODE = few epochs, 1 seed, tiny sweep; FULL_MODE = multi-seed ([0,1,2]), wider sweep, longer epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f6e6c",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "Clone or reuse the repo, install requirements, set PYTHONPATH, and print environment versions. Colab-safe with optional Drive mount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4783010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, json, datetime, platform\n",
    "from pathlib import Path\n",
    "\n",
    "COLAB = \"google.colab\" in sys.modules\n",
    "REPO_URL = \"https://github.com/kaminglui/Domain-Adaptation-with-ME-IIS.git\"\n",
    "DEFAULT_REPO_DIR = Path(\"/content/Domain-Adaptation-with-ME-IIS\") if COLAB else Path.cwd()\n",
    "REPO_DIR = Path(os.getenv(\"REPO_DIR\", DEFAULT_REPO_DIR))\n",
    "\n",
    "if COLAB:\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        if os.getenv(\"MOUNT_DRIVE\", \"1\") == \"1\":\n",
    "            drive.mount(\"/content/drive\")\n",
    "    except Exception as exc:\n",
    "        print(\"[Drive] Skipping Drive mount:\", exc)\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[Repo] Cloning into {REPO_DIR} ...\")\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "else:\n",
    "    print(f\"[Repo] Using existing repo at {REPO_DIR}\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "if str(REPO_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "req_file = Path(\"env/requirements_colab.txt\") if Path(\"env/requirements_colab.txt\").exists() else Path(\"requirements.txt\")\n",
    "print(\"[Setup] Installing dependencies from\", req_file)\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)], check=True)\n",
    "\n",
    "import torch, numpy as np, sklearn\n",
    "print({\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"torch\": torch.__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"sklearn\": sklearn.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "})\n",
    "\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(\"[Setup] Deterministic flags applied.\")\n",
    "except Exception as exc:\n",
    "    print(\"[Setup][WARN] Could not enable full determinism:\", exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1f4cc",
   "metadata": {},
   "source": [
    "## 2) Data setup (Office-Home / Office-31)\n",
    "- Mount Google Drive if storing datasets there (see Setup cell).\n",
    "- Set `DATA_ROOT` below; no auto-download is performed here.\n",
    "- Expected Office-Home domains: Art, Clipart, Product, Real_World/RealWorld/Real World.\n",
    "- Target labels are used only for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8800ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(os.getenv(\"DATA_ROOT\", \"datasets/Office-Home\")).expanduser()\n",
    "EXPECTED_DOMAINS = [\"Art\", \"Clipart\", \"Product\"]\n",
    "REAL_DOMAIN_CANDIDATES = [\"Real_World\", \"RealWorld\", \"Real World\", \"Real\"]\n",
    "print(f\"[Data] DATA_ROOT set to: {DATA_ROOT}\")\n",
    "if not DATA_ROOT.exists():\n",
    "    print(\"[Data][WARN] DATA_ROOT does not exist. Edit this cell or the config cell below.\")\n",
    "else:\n",
    "    missing = [d for d in EXPECTED_DOMAINS if not (DATA_ROOT / d).exists()]\n",
    "    has_real = any((DATA_ROOT / d).exists() for d in REAL_DOMAIN_CANDIDATES)\n",
    "    if missing or not has_real:\n",
    "        print(f\"[Data][WARN] Found path but missing expected folders. Missing={missing}, has_real={has_real}\")\n",
    "    else:\n",
    "        print(\"[Data] Domain folders detected. Ready for training/adaptation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade06654",
   "metadata": {},
   "source": [
    "## 3) Single config (edit only here)\n",
    "Set QUICK_MODE/FULL_MODE, dataset/domains, feature layers, backend params, seeds, and run toggles. Seeds=[0] in quick, [0,1,2] in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "QUICK_MODE = True  # flip to False for FULL_MODE\n",
    "\n",
    "CFG = {\n",
    "    \"quick_mode\": QUICK_MODE,\n",
    "    \"dataset_name\": \"office_home\",  # or office31\n",
    "    \"data_root\": str(DATA_ROOT.resolve() if DATA_ROOT.exists() else DATA_ROOT),\n",
    "    \"source_domain\": \"Ar\",\n",
    "    \"target_domain\": \"Cl\",\n",
    "    \"feature_layers\": [\"layer3\", \"layer4\"],\n",
    "    \"components_per_layer\": 5,\n",
    "    \"components_override\": \"\",  # optional e.g., \"layer3:8,layer4:6\"\n",
    "    \"cluster_backend\": \"gmm\",  # default; per-run overrides below\n",
    "    \"gmm_selection_mode\": \"fixed\",  # or \"bic\"\n",
    "    \"vmf_kappa\": 20.0,\n",
    "    \"cluster_clean_ratio\": 1.0,\n",
    "    \"kmeans_n_init\": 10,\n",
    "    \"num_latent_styles\": 5,\n",
    "    \"batch_size\": 16 if QUICK_MODE else 32,\n",
    "    \"num_workers\": 2 if QUICK_MODE else 4,\n",
    "    \"train_epochs\": 2 if QUICK_MODE else 50,\n",
    "    \"adapt_epochs\": 2 if QUICK_MODE else 10,\n",
    "    \"lr_backbone\": 1e-3,\n",
    "    \"lr_classifier\": 1e-2,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"iis_iters\": 5 if QUICK_MODE else 15,\n",
    "    \"iis_tol\": 1e-3,\n",
    "    \"source_prob_mode\": \"softmax\",  # or onehot\n",
    "    \"finetune_backbone\": False,\n",
    "    \"backbone_lr_scale\": 0.1,\n",
    "    \"use_pseudo_labels_stage2\": False,\n",
    "    \"pseudo_conf_thresh\": 0.9,\n",
    "    \"pseudo_max_ratio\": 1.0,\n",
    "    \"pseudo_loss_weight\": 1.0,\n",
    "    \"stage2_epochs\": 1 if QUICK_MODE else 5,\n",
    "    \"force_rerun\": False,\n",
    "    \"deterministic\": True,\n",
    "    \"output_root\": str(Path(\"outputs\")),\n",
    "    \"run_tag\": None,\n",
    "    \"seeds_quick\": [0],\n",
    "    \"seeds_full\": [0, 1, 2],\n",
    "    \"run_source_train\": True,\n",
    "    \"run_baselines\": True,\n",
    "    \"run_backend_compare\": True,\n",
    "    \"run_sweep\": False,\n",
    "    \"run_layers_ablation\": False,\n",
    "    \"run_diagnostics\": False,\n",
    "    \"run_experiment_driver\": False,\n",
    "    \"sweep_grid_quick\": {\"K\": [5, 10], \"kappa\": [10.0, 20.0], \"clean_ratio\": [1.0, 0.8]},\n",
    "    \"sweep_grid_full\": {\"K\": [5, 10, 20], \"kappa\": [10.0, 20.0, 40.0], \"clean_ratio\": [1.0, 0.8, 0.6]},\n",
    "    \"layer_sets\": [[\"layer4\"], [\"layer3\", \"layer4\"]],\n",
    "}\n",
    "\n",
    "CFG[\"seeds\"] = CFG[\"seeds_quick\"] if QUICK_MODE else CFG[\"seeds_full\"]\n",
    "print(json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de6b89",
   "metadata": {},
   "source": [
    "## 4) Helper utilities\n",
    "Minimal helpers to launch scripts (train_source.py, adapt_me_iis.py), manage run directories, and load results without duplicating ME-IIS logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17893f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, subprocess, datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from utils.experiment_utils import dataset_tag\n",
    "\n",
    "PYTHON = sys.executable\n",
    "RUN_MODE = \"quick\" if CFG[\"quick_mode\"] else \"full\"\n",
    "RUN_TS = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_TAG = CFG[\"run_tag\"] or f\"{RUN_MODE}_{CFG['source_domain']}2{CFG['target_domain']}_{RUN_TS}\"\n",
    "RUN_ROOT = Path(CFG[\"output_root\"]).expanduser() / \"runs\" / RUN_TAG\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[RUN] Output root: {RUN_ROOT}\")\n",
    "\n",
    "def feature_layers_str(layers: List[str]) -> str:\n",
    "    return \" ,\".join(layers).replace(\" ,\", \",\")\n",
    "\n",
    "def make_workdir(method_tag: str, seed: int, extra: str = \"\") -> Path:\n",
    "    parts = [method_tag, f\"seed{seed}\"]\n",
    "    if extra:\n",
    "        parts.append(extra)\n",
    "    workdir = RUN_ROOT / \"_\".join(parts)\n",
    "    (workdir / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "    (workdir / \"configs\").mkdir(parents=True, exist_ok=True)\n",
    "    return workdir\n",
    "\n",
    "def run_cmd(args: List[str], workdir: Path, log_name: str = \"run.log\") -> subprocess.CompletedProcess:\n",
    "    workdir = Path(workdir)\n",
    "    log_path = workdir / \"logs\" / log_name\n",
    "    env = os.environ.copy()\n",
    "    env[\"PYTHONPATH\"] = os.pathsep.join([str(REPO_DIR), env.get(\"PYTHONPATH\", \"\")])\n",
    "    print(\"[RUN]\", \" \".join(args))\n",
    "    proc = subprocess.run(args, cwd=workdir, env=env, text=True, capture_output=True)\n",
    "    log_path.write_text(proc.stdout + \"\n",
    "\n",
    "[stderr]\n",
    "\" + proc.stderr)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed (exit={proc.returncode}). See log: {log_path}\")\n",
    "    return proc\n",
    "\n",
    "def load_results_csv(paths: Optional[List[Path]] = None) -> pd.DataFrame:\n",
    "    if paths is None:\n",
    "        paths = list(RUN_ROOT.rglob(\"office_home_me_iis.csv\"))\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            df[\"csv_path\"] = str(p)\n",
    "            frames.append(df)\n",
    "        except Exception as exc:\n",
    "            print(f\"[WARN] Could not read {p}: {exc}\")\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def existing_row(csv_path: Path, seed: int, method: Optional[str] = None) -> Optional[pd.Series]:\n",
    "    if not csv_path.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    mask = (\n",
    "        (df[\"dataset\"] == dataset_tag(CFG[\"dataset_name\"]))\n",
    "        & (df[\"source\"] == CFG[\"source_domain\"])\n",
    "        & (df[\"target\"] == CFG[\"target_domain\"])\n",
    "        & (df[\"seed\"] == seed)\n",
    "    )\n",
    "    if method is not None:\n",
    "        mask &= df[\"method\"] == method\n",
    "    if not mask.any():\n",
    "        return None\n",
    "    return df[mask].iloc[-1]\n",
    "\n",
    "RUN_LOG: List[Dict] = []\n",
    "\n",
    "def record_result(method_label: str, backend: str, seed: int, run_dir: Path) -> Optional[Dict]:\n",
    "    csv_path = Path(run_dir) / \"results\" / \"office_home_me_iis.csv\"\n",
    "    row = existing_row(csv_path, seed)\n",
    "    if row is None:\n",
    "        print(f\"[WARN] No matching row found in {csv_path} for seed={seed}\")\n",
    "        return None\n",
    "    rec = row.to_dict()\n",
    "    rec.update({\"method_label\": method_label, \"backend\": backend, \"run_dir\": str(run_dir)})\n",
    "    RUN_LOG.append(rec)\n",
    "    return rec\n",
    "\n",
    "def source_ckpt_name(seed: int) -> str:\n",
    "    return f\"source_only_{CFG['source_domain']}_to_{CFG['target_domain']}_seed{seed}.pth\"\n",
    "\n",
    "def adapt_ckpt_name(seed: int, layers: str) -> str:\n",
    "    tag = layers.replace(\",\", \"-\").replace(\" \", \"\")\n",
    "    return f\"me_iis_{CFG['source_domain']}_to_{CFG['target_domain']}_{tag}_seed{seed}.pth\"\n",
    "\n",
    "print(f\"[RUN] Seeds: {CFG['seeds']}, quick_mode={CFG['quick_mode']}, deterministic={CFG['deterministic']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee508146",
   "metadata": {},
   "source": [
    "## 5) Baseline runs (source-only, ME-IIS GMM, ME-IIS vMF)\n",
    "For each seed: train source-only (skip if checkpoint exists), then adapt with GMM and vMF. Results append to per-run CSVs and are aggregated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "feature_str = feature_layers_str(CFG[\"feature_layers\"])\n",
    "baseline_rows = []\n",
    "\n",
    "quick_train_flags = [\"--dry_run_max_batches\", \"2\", \"--dry_run_max_samples\", \"8\"] if CFG[\"quick_mode\"] else []\n",
    "quick_adapt_flags = [\"--dry_run_max_batches\", \"2\", \"--dry_run_max_samples\", \"8\"] if CFG[\"quick_mode\"] else []\n",
    "\n",
    "for seed in CFG[\"seeds\"]:\n",
    "    # Train source-only\n",
    "    src_dir = make_workdir(\"source_only\", seed)\n",
    "    src_ckpt = src_dir / \"checkpoints\" / source_ckpt_name(seed)\n",
    "    if CFG[\"run_source_train\"] and (CFG[\"force_rerun\"] or not src_ckpt.exists()):\n",
    "        args = [\n",
    "            PYTHON,\n",
    "            str(REPO_DIR / \"scripts\" / \"train_source.py\"),\n",
    "            \"--dataset_name\",\n",
    "            CFG[\"dataset_name\"],\n",
    "            \"--data_root\",\n",
    "            CFG[\"data_root\"],\n",
    "            \"--source_domain\",\n",
    "            CFG[\"source_domain\"],\n",
    "            \"--target_domain\",\n",
    "            CFG[\"target_domain\"],\n",
    "            \"--num_epochs\",\n",
    "            str(CFG[\"train_epochs\"]),\n",
    "            \"--batch_size\",\n",
    "            str(CFG[\"batch_size\"]),\n",
    "            \"--lr_backbone\",\n",
    "            str(CFG[\"lr_backbone\"]),\n",
    "            \"--lr_classifier\",\n",
    "            str(CFG[\"lr_classifier\"]),\n",
    "            \"--weight_decay\",\n",
    "            str(CFG[\"weight_decay\"]),\n",
    "            \"--num_workers\",\n",
    "            str(CFG[\"num_workers\"]),\n",
    "            \"--seed\",\n",
    "            str(seed),\n",
    "            \"--dump_config\",\n",
    "            str(src_dir / \"configs\" / f\"train_seed{seed}.json\"),\n",
    "        ]\n",
    "        if CFG[\"deterministic\"]:\n",
    "            args.append(\"--deterministic\")\n",
    "        args += quick_train_flags\n",
    "        run_cmd(args, workdir=src_dir, log_name=\"train_source.txt\")\n",
    "    else:\n",
    "        print(f\"[Skip] Source training for seed={seed} (checkpoint exists and force_rerun={CFG['force_rerun']}).\")\n",
    "\n",
    "    src_row = record_result(\"source_only\", \"source_only\", seed, src_dir)\n",
    "    if src_row is not None:\n",
    "        baseline_rows.append(src_row)\n",
    "\n",
    "    if not CFG[\"run_baselines\"]:\n",
    "        continue\n",
    "\n",
    "    # ME-IIS GMM\n",
    "    gmm_dir = make_workdir(\"me_iis_gmm\", seed)\n",
    "    gmm_ckpt = gmm_dir / \"checkpoints\" / adapt_ckpt_name(seed, feature_str)\n",
    "    gmm_args = [\n",
    "        PYTHON,\n",
    "        str(REPO_DIR / \"scripts\" / \"adapt_me_iis.py\"),\n",
    "        \"--dataset_name\",\n",
    "        CFG[\"dataset_name\"],\n",
    "        \"--data_root\",\n",
    "        CFG[\"data_root\"],\n",
    "        \"--source_domain\",\n",
    "        CFG[\"source_domain\"],\n",
    "        \"--target_domain\",\n",
    "        CFG[\"target_domain\"],\n",
    "        \"--checkpoint\",\n",
    "        str(src_ckpt.resolve()),\n",
    "        \"--batch_size\",\n",
    "        str(CFG[\"batch_size\"]),\n",
    "        \"--num_workers\",\n",
    "        str(CFG[\"num_workers\"]),\n",
    "        \"--feature_layers\",\n",
    "        feature_str,\n",
    "        \"--num_latent_styles\",\n",
    "        str(CFG[\"components_per_layer\"]),\n",
    "        \"--gmm_selection_mode\",\n",
    "        CFG[\"gmm_selection_mode\"],\n",
    "        \"--iis_iters\",\n",
    "        str(CFG[\"iis_iters\"]),\n",
    "        \"--iis_tol\",\n",
    "        str(CFG[\"iis_tol\"]),\n",
    "        \"--adapt_epochs\",\n",
    "        str(CFG[\"adapt_epochs\"]),\n",
    "        \"--classifier_lr\",\n",
    "        str(CFG[\"lr_classifier\"]),\n",
    "        \"--weight_decay\",\n",
    "        str(CFG[\"weight_decay\"]),\n",
    "        \"--backbone_lr_scale\",\n",
    "        str(CFG[\"backbone_lr_scale\"]),\n",
    "        \"--source_prob_mode\",\n",
    "        CFG[\"source_prob_mode\"],\n",
    "        \"--cluster_backend\",\n",
    "        \"gmm\",\n",
    "        \"--dump_config\",\n",
    "        str(gmm_dir / \"configs\" / f\"adapt_gmm_seed{seed}.json\"),\n",
    "    ]\n",
    "    if CFG[\"finetune_backbone\"]:\n",
    "        gmm_args.append(\"--finetune_backbone\")\n",
    "    if CFG[\"components_override\"].strip():\n",
    "        gmm_args += [\"--components_per_layer\", CFG[\"components_override\"].strip()]\n",
    "    if CFG[\"deterministic\"]:\n",
    "        gmm_args.append(\"--deterministic\")\n",
    "    gmm_args += quick_adapt_flags\n",
    "    if CFG[\"force_rerun\"] or not gmm_ckpt.exists():\n",
    "        run_cmd(gmm_args, workdir=gmm_dir, log_name=\"adapt_gmm.txt\")\n",
    "    else:\n",
    "        print(f\"[Skip] GMM adaptation for seed={seed} (checkpoint exists and force_rerun={CFG['force_rerun']}).\")\n",
    "    gmm_row = record_result(\"me_iis_gmm\", \"gmm\", seed, gmm_dir)\n",
    "    if gmm_row is not None:\n",
    "        baseline_rows.append(gmm_row)\n",
    "\n",
    "    # ME-IIS vMF-softmax\n",
    "    if CFG[\"run_backend_compare\"]:\n",
    "        vmf_dir = make_workdir(\"me_iis_vmf\", seed)\n",
    "        vmf_ckpt = vmf_dir / \"checkpoints\" / adapt_ckpt_name(seed, feature_str)\n",
    "        vmf_args = [\n",
    "            PYTHON,\n",
    "            str(REPO_DIR / \"scripts\" / \"adapt_me_iis.py\"),\n",
    "            \"--dataset_name\",\n",
    "            CFG[\"dataset_name\"],\n",
    "            \"--data_root\",\n",
    "            CFG[\"data_root\"],\n",
    "            \"--source_domain\",\n",
    "            CFG[\"source_domain\"],\n",
    "            \"--target_domain\",\n",
    "            CFG[\"target_domain\"],\n",
    "            \"--checkpoint\",\n",
    "            str(src_ckpt.resolve()),\n",
    "            \"--batch_size\",\n",
    "            str(CFG[\"batch_size\"]),\n",
    "            \"--num_workers\",\n",
    "            str(CFG[\"num_workers\"]),\n",
    "            \"--feature_layers\",\n",
    "            feature_str,\n",
    "            \"--num_latent_styles\",\n",
    "            str(CFG[\"components_per_layer\"]),\n",
    "            \"--cluster_backend\",\n",
    "            \"vmf_softmax\",\n",
    "            \"--vmf_kappa\",\n",
    "            str(CFG[\"vmf_kappa\"]),\n",
    "            \"--cluster_clean_ratio\",\n",
    "            str(CFG[\"cluster_clean_ratio\"]),\n",
    "            \"--kmeans_n_init\",\n",
    "            str(CFG[\"kmeans_n_init\"]),\n",
    "            \"--iis_iters\",\n",
    "            str(CFG[\"iis_iters\"]),\n",
    "            \"--iis_tol\",\n",
    "            str(CFG[\"iis_tol\"]),\n",
    "            \"--adapt_epochs\",\n",
    "            str(CFG[\"adapt_epochs\"]),\n",
    "            \"--classifier_lr\",\n",
    "            str(CFG[\"lr_classifier\"]),\n",
    "            \"--weight_decay\",\n",
    "            str(CFG[\"weight_decay\"]),\n",
    "            \"--backbone_lr_scale\",\n",
    "            str(CFG[\"backbone_lr_scale\"]),\n",
    "            \"--source_prob_mode\",\n",
    "            CFG[\"source_prob_mode\"],\n",
    "            \"--dump_config\",\n",
    "            str(vmf_dir / \"configs\" / f\"adapt_vmf_seed{seed}.json\"),\n",
    "        ]\n",
    "        if CFG[\"finetune_backbone\"]:\n",
    "            vmf_args.append(\"--finetune_backbone\")\n",
    "        if CFG[\"components_override\"].strip():\n",
    "            vmf_args += [\"--components_per_layer\", CFG[\"components_override\"].strip()]\n",
    "        if CFG[\"deterministic\"]:\n",
    "            vmf_args.append(\"--deterministic\")\n",
    "        vmf_args += quick_adapt_flags\n",
    "        if CFG[\"force_rerun\"] or not vmf_ckpt.exists():\n",
    "            run_cmd(vmf_args, workdir=vmf_dir, log_name=\"adapt_vmf.txt\")\n",
    "        else:\n",
    "            print(f\"[Skip] vMF adaptation for seed={seed} (checkpoint exists and force_rerun={CFG['force_rerun']}).\")\n",
    "\n",
    "        vmf_row = record_result(\"me_iis_vmf\", \"vmf_softmax\", seed, vmf_dir)\n",
    "        if vmf_row is not None:\n",
    "            baseline_rows.append(vmf_row)\n",
    "\n",
    "        if CFG[\"use_pseudo_labels_stage2\"]:\n",
    "            pl_row = existing_row(vmf_dir / \"results\" / \"office_home_me_iis.csv\", seed, method=\"me_iis_pl\")\n",
    "            if CFG[\"force_rerun\"] or pl_row is None:\n",
    "                pl_args = list(vmf_args)\n",
    "                pl_args += [\n",
    "                    \"--use_pseudo_labels\",\n",
    "                    \"--pseudo_conf_thresh\",\n",
    "                    str(CFG[\"pseudo_conf_thresh\"]),\n",
    "                    \"--pseudo_max_ratio\",\n",
    "                    str(CFG[\"pseudo_max_ratio\"]),\n",
    "                    \"--pseudo_loss_weight\",\n",
    "                    str(CFG[\"pseudo_loss_weight\"]),\n",
    "                    \"--adapt_epochs\",\n",
    "                    str(CFG[\"stage2_epochs\"]),\n",
    "                    \"--dump_config\",\n",
    "                    str(vmf_dir / \"configs\" / f\"adapt_vmf_pl_seed{seed}.json\"),\n",
    "                ]\n",
    "                run_cmd(pl_args, workdir=vmf_dir, log_name=\"adapt_vmf_pseudo.txt\")\n",
    "            pl_row = existing_row(vmf_dir / \"results\" / \"office_home_me_iis.csv\", seed, method=\"me_iis_pl\")\n",
    "            if pl_row is not None:\n",
    "                rec = pl_row.to_dict()\n",
    "                rec.update({\"method_label\": \"me_iis_vmf_pl\", \"backend\": \"vmf_softmax\", \"run_dir\": str(vmf_dir)})\n",
    "                RUN_LOG.append(rec)\n",
    "                baseline_rows.append(rec)\n",
    "\n",
    "if CFG.get(\"run_experiment_driver\"):\n",
    "    driver_dir = make_workdir(\"experiment_driver\", CFG[\"seeds\"][0], extra=\"layers\")\n",
    "    (driver_dir / \"results\").mkdir(parents=True, exist_ok=True)\n",
    "    seeds_str = \",\".join(str(s) for s in CFG[\"seeds\"])\n",
    "    driver_args = [\n",
    "        PYTHON,\n",
    "        str(REPO_DIR / \"scripts\" / \"run_me_iis_experiments.py\"),\n",
    "        \"--dataset_name\",\n",
    "        CFG[\"dataset_name\"],\n",
    "        \"--source_domain\",\n",
    "        CFG[\"source_domain\"],\n",
    "        \"--target_domain\",\n",
    "        CFG[\"target_domain\"],\n",
    "        \"--experiment_family\",\n",
    "        \"layers\",\n",
    "        \"--seeds\",\n",
    "        seeds_str,\n",
    "        \"--base_data_root\",\n",
    "        CFG[\"data_root\"],\n",
    "        \"--feature_layers\",\n",
    "        feature_str,\n",
    "        \"--num_latent_styles\",\n",
    "        str(CFG[\"components_per_layer\"]),\n",
    "        \"--gmm_selection_mode\",\n",
    "        CFG[\"gmm_selection_mode\"],\n",
    "        \"--num_epochs\",\n",
    "        str(CFG[\"train_epochs\"]),\n",
    "        \"--batch_size\",\n",
    "        str(CFG[\"batch_size\"]),\n",
    "        \"--num_workers\",\n",
    "        str(CFG[\"num_workers\"]),\n",
    "        \"--iis_iters\",\n",
    "        str(CFG[\"iis_iters\"]),\n",
    "        \"--iis_tol\",\n",
    "        str(CFG[\"iis_tol\"]),\n",
    "        \"--adapt_epochs\",\n",
    "        str(CFG[\"adapt_epochs\"]),\n",
    "        \"--backbone_lr_scale\",\n",
    "        str(CFG[\"backbone_lr_scale\"]),\n",
    "        \"--classifier_lr\",\n",
    "        str(CFG[\"lr_classifier\"]),\n",
    "        \"--weight_decay\",\n",
    "        str(CFG[\"weight_decay\"]),\n",
    "        \"--source_prob_mode\",\n",
    "        CFG[\"source_prob_mode\"],\n",
    "        \"--output_csv\",\n",
    "        str(driver_dir / \"results\" / f\"experiments_{CFG['source_domain']}2{CFG['target_domain']}.csv\"),\n",
    "    ]\n",
    "    if CFG[\"deterministic\"]:\n",
    "        driver_args.append(\"--deterministic\")\n",
    "    if CFG[\"components_override\"].strip():\n",
    "        driver_args += [\"--components_per_layer\", CFG[\"components_override\"].strip()]\n",
    "    driver_args += quick_adapt_flags\n",
    "    run_cmd(driver_args, workdir=driver_dir, log_name=\"run_experiment_driver.txt\")\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_rows)\n",
    "if not baseline_df.empty:\n",
    "    display(baseline_df)\n",
    "else:\n",
    "    print(\"[Info] No baseline rows collected yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec27abf",
   "metadata": {},
   "source": [
    "## 6) vMF hyperparameter sweep (optional)\n",
    "Tiny grid in QUICK_MODE; wider grid in FULL_MODE. Uses the first seed and reuses the source checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if CFG[\"run_sweep\"]:\n",
    "    seed = CFG[\"seeds\"][0]\n",
    "    sweep_dir = RUN_ROOT / \"vmf_sweep\"\n",
    "    sweep_dir.mkdir(parents=True, exist_ok=True)\n",
    "    src_dir = make_workdir(\"source_only\", seed)\n",
    "    src_ckpt = src_dir / \"checkpoints\" / source_ckpt_name(seed)\n",
    "    if not src_ckpt.exists():\n",
    "        print(\"[Sweep] Source checkpoint missing; training a quick source model...\")\n",
    "        quick_args = [\n",
    "            PYTHON,\n",
    "            str(REPO_DIR / \"scripts\" / \"train_source.py\"),\n",
    "            \"--dataset_name\",\n",
    "            CFG[\"dataset_name\"],\n",
    "            \"--data_root\",\n",
    "            CFG[\"data_root\"],\n",
    "            \"--source_domain\",\n",
    "            CFG[\"source_domain\"],\n",
    "            \"--target_domain\",\n",
    "            CFG[\"target_domain\"],\n",
    "            \"--num_epochs\",\n",
    "            str(CFG[\"train_epochs\"]),\n",
    "            \"--batch_size\",\n",
    "            str(CFG[\"batch_size\"]),\n",
    "            \"--seed\",\n",
    "            str(seed),\n",
    "            \"--dump_config\",\n",
    "            str(src_dir / \"configs\" / f\"train_seed{seed}.json\"),\n",
    "        ]\n",
    "        if CFG[\"deterministic\"]:\n",
    "            quick_args.append(\"--deterministic\")\n",
    "        quick_args += [\"--dry_run_max_batches\", \"2\", \"--dry_run_max_samples\", \"8\"] if CFG[\"quick_mode\"] else []\n",
    "        run_cmd(quick_args, workdir=src_dir, log_name=\"train_source_for_sweep.txt\")\n",
    "\n",
    "    grid = CFG[\"sweep_grid_quick\"] if CFG[\"quick_mode\"] else CFG[\"sweep_grid_full\"]\n",
    "    sweep_rows = []\n",
    "    for K in grid.get(\"K\", []):\n",
    "        for kappa in grid.get(\"kappa\", []):\n",
    "            for clean in grid.get(\"clean_ratio\", []):\n",
    "                tag = f\"K{K}_k{kappa}_c{clean}\"\n",
    "                run_dir = sweep_dir / f\"{tag}_seed{seed}\"\n",
    "                (run_dir / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "                (run_dir / \"configs\").mkdir(parents=True, exist_ok=True)\n",
    "                args = [\n",
    "                    PYTHON,\n",
    "                    str(REPO_DIR / \"scripts\" / \"adapt_me_iis.py\"),\n",
    "                    \"--dataset_name\",\n",
    "                    CFG[\"dataset_name\"],\n",
    "                    \"--data_root\",\n",
    "                    CFG[\"data_root\"],\n",
    "                    \"--source_domain\",\n",
    "                    CFG[\"source_domain\"],\n",
    "                    \"--target_domain\",\n",
    "                    CFG[\"target_domain\"],\n",
    "                    \"--checkpoint\",\n",
    "                    str(src_ckpt.resolve()),\n",
    "                    \"--batch_size\",\n",
    "                    str(CFG[\"batch_size\"]),\n",
    "                    \"--num_workers\",\n",
    "                    str(CFG[\"num_workers\"]),\n",
    "                    \"--feature_layers\",\n",
    "                    feature_layers_str(CFG[\"feature_layers\"]),\n",
    "                    \"--num_latent_styles\",\n",
    "                    str(K),\n",
    "                    \"--cluster_backend\",\n",
    "                    \"vmf_softmax\",\n",
    "                    \"--vmf_kappa\",\n",
    "                    str(float(kappa)),\n",
    "                    \"--cluster_clean_ratio\",\n",
    "                    str(float(clean)),\n",
    "                    \"--kmeans_n_init\",\n",
    "                    str(CFG[\"kmeans_n_init\"]),\n",
    "                    \"--iis_iters\",\n",
    "                    str(CFG[\"iis_iters\"]),\n",
    "                    \"--iis_tol\",\n",
    "                    str(CFG[\"iis_tol\"]),\n",
    "                    \"--adapt_epochs\",\n",
    "                    str(CFG[\"adapt_epochs\"]),\n",
    "                    \"--classifier_lr\",\n",
    "                    str(CFG[\"lr_classifier\"]),\n",
    "                    \"--weight_decay\",\n",
    "                    str(CFG[\"weight_decay\"]),\n",
    "                    \"--backbone_lr_scale\",\n",
    "                    str(CFG[\"backbone_lr_scale\"]),\n",
    "                    \"--source_prob_mode\",\n",
    "                    CFG[\"source_prob_mode\"],\n",
    "                    \"--dump_config\",\n",
    "                    str(run_dir / \"configs\" / f\"adapt_vmf_{tag}.json\"),\n",
    "                ]\n",
    "                if CFG[\"deterministic\"]:\n",
    "                    args.append(\"--deterministic\")\n",
    "                args += [\"--dry_run_max_batches\", \"2\", \"--dry_run_max_samples\", \"8\"] if CFG[\"quick_mode\"] else []\n",
    "                run_cmd(args, workdir=run_dir, log_name=\"adapt_vmf_sweep.txt\")\n",
    "                row = record_result(f\"vmf_sweep_{tag}\", \"vmf_softmax\", seed, run_dir)\n",
    "                if row is not None:\n",
    "                    row.update({\"K\": K, \"kappa\": kappa, \"clean_ratio\": clean})\n",
    "                    sweep_rows.append(row)\n",
    "    sweep_df = pd.DataFrame(sweep_rows)\n",
    "    sweep_csv = sweep_dir / \"vmf_sweep.csv\"\n",
    "    if not sweep_df.empty:\n",
    "        sweep_df.to_csv(sweep_csv, index=False)\n",
    "        display(sweep_df.sort_values(\"target_acc\", ascending=False).head(10))\n",
    "    else:\n",
    "        print(\"[Sweep] No sweep rows to show.\")\n",
    "else:\n",
    "    print(\"RUN_SWEEP is False; skipping vMF sweep.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5735e8",
   "metadata": {},
   "source": [
    "## 7) Layers ablation (optional)\n",
    "Compares feature layer choices across GMM and vMF backends using the configured seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f919260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if CFG[\"run_layers_ablation\"]:\n",
    "    ablation_rows = []\n",
    "    for seed in CFG[\"seeds\"]:\n",
    "        src_dir = make_workdir(\"source_only\", seed)\n",
    "        src_ckpt = src_dir / \"checkpoints\" / source_ckpt_name(seed)\n",
    "        if not src_ckpt.exists():\n",
    "            print(f\"[Ablation][WARN] Missing source checkpoint for seed={seed}; run baselines first.\")\n",
    "            continue\n",
    "        for layers in CFG.get(\"layer_sets\", []):\n",
    "            layers_str = feature_layers_str(layers)\n",
    "            for backend in [\"gmm\", \"vmf_softmax\"]:\n",
    "                run_dir = RUN_ROOT / f\"ablation_{backend}_seed{seed}_{layers_str.replace(',', '-') }\"\n",
    "                (run_dir / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "                (run_dir / \"configs\").mkdir(parents=True, exist_ok=True)\n",
    "                ckpt = run_dir / \"checkpoints\" / adapt_ckpt_name(seed, layers_str)\n",
    "                args = [\n",
    "                    PYTHON,\n",
    "                    str(REPO_DIR / \"scripts\" / \"adapt_me_iis.py\"),\n",
    "                    \"--dataset_name\",\n",
    "                    CFG[\"dataset_name\"],\n",
    "                    \"--data_root\",\n",
    "                    CFG[\"data_root\"],\n",
    "                    \"--source_domain\",\n",
    "                    CFG[\"source_domain\"],\n",
    "                    \"--target_domain\",\n",
    "                    CFG[\"target_domain\"],\n",
    "                    \"--checkpoint\",\n",
    "                    str(src_ckpt.resolve()),\n",
    "                    \"--batch_size\",\n",
    "                    str(CFG[\"batch_size\"]),\n",
    "                    \"--num_workers\",\n",
    "                    str(CFG[\"num_workers\"]),\n",
    "                    \"--feature_layers\",\n",
    "                    layers_str,\n",
    "                    \"--num_latent_styles\",\n",
    "                    str(CFG[\"components_per_layer\"]),\n",
    "                    \"--cluster_backend\",\n",
    "                    backend,\n",
    "                    \"--vmf_kappa\",\n",
    "                    str(CFG[\"vmf_kappa\"]),\n",
    "                    \"--cluster_clean_ratio\",\n",
    "                    str(CFG[\"cluster_clean_ratio\"]),\n",
    "                    \"--kmeans_n_init\",\n",
    "                    str(CFG[\"kmeans_n_init\"]),\n",
    "                    \"--gmm_selection_mode\",\n",
    "                    CFG[\"gmm_selection_mode\"],\n",
    "                    \"--iis_iters\",\n",
    "                    str(CFG[\"iis_iters\"]),\n",
    "                    \"--iis_tol\",\n",
    "                    str(CFG[\"iis_tol\"]),\n",
    "                    \"--adapt_epochs\",\n",
    "                    str(CFG[\"adapt_epochs\"]),\n",
    "                    \"--classifier_lr\",\n",
    "                    str(CFG[\"lr_classifier\"]),\n",
    "                    \"--weight_decay\",\n",
    "                    str(CFG[\"weight_decay\"]),\n",
    "                    \"--backbone_lr_scale\",\n",
    "                    str(CFG[\"backbone_lr_scale\"]),\n",
    "                    \"--source_prob_mode\",\n",
    "                    CFG[\"source_prob_mode\"],\n",
    "                    \"--dump_config\",\n",
    "                    str(run_dir / \"configs\" / f\"adapt_{backend}_{layers_str}_seed{seed}.json\"),\n",
    "                ]\n",
    "                if CFG[\"deterministic\"]:\n",
    "                    args.append(\"--deterministic\")\n",
    "                args += [\"--dry_run_max_batches\", \"2\", \"--dry_run_max_samples\", \"8\"] if CFG[\"quick_mode\"] else []\n",
    "                if CFG[\"force_rerun\"] or not ckpt.exists():\n",
    "                    run_cmd(args, workdir=run_dir, log_name=\"adapt_ablation.txt\")\n",
    "                row = record_result(f\"ablation_{backend}_{layers_str}\", backend, seed, run_dir)\n",
    "                if row is not None:\n",
    "                    row.update({\"layers\": layers_str, \"backend\": backend})\n",
    "                    ablation_rows.append(row)\n",
    "    ablation_df = pd.DataFrame(ablation_rows)\n",
    "    if not ablation_df.empty:\n",
    "        display(ablation_df)\n",
    "    else:\n",
    "        print(\"[Ablation] No rows recorded.\")\n",
    "else:\n",
    "    print(\"RUN_LAYERS_ABLATION is False; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942ad7e",
   "metadata": {},
   "source": [
    "## 8) Diagnostics + sanity checks (optional)\n",
    "Loads saved IIS artifacts where available and performs quick PMF/mass checks plus convergence plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92adad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from clustering.factory import create_backend\n",
    "from models.me_iis_adapter import MaxEntAdapter\n",
    "\n",
    "if not CFG[\"run_diagnostics\"]:\n",
    "    print(\"RUN_DIAGNOSTICS is False; skipping diagnostics.\")\n",
    "else:\n",
    "    npz_candidates = sorted(RUN_ROOT.glob(\"**/results/me_iis_weights_*.npz\"))\n",
    "    if not npz_candidates:\n",
    "        print(\"[Diag][WARN] No IIS history npz files found under run root.\")\n",
    "    else:\n",
    "        npz_path = npz_candidates[-1]\n",
    "        print(f\"[Diag] Using npz: {npz_path}\")\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        if \"weights\" in data:\n",
    "            w = data[\"weights\"]\n",
    "            print(f\"[Diag] weights stats min={w.min():.4e} mean={w.mean():.4e} max={w.max():.4e} sum={w.sum():.4f}\")\n",
    "        if \"feature_mass_mean\" in data:\n",
    "            expected_mass = len(CFG[\"feature_layers\"])\n",
    "            fm_mean = float(data[\"feature_mass_mean\"][-1])\n",
    "            fm_min = float(data[\"feature_mass_min\"][-1])\n",
    "            fm_max = float(data[\"feature_mass_max\"][-1])\n",
    "            print(f\"[Diag] feature mass mean={fm_mean:.4f} (expected ~{expected_mass}), min={fm_min:.4f}, max={fm_max:.4f}\")\n",
    "        if \"moment_max\" in data:\n",
    "            plt.figure(figsize=(5,3))\n",
    "            plt.plot(data[\"moment_max\"], label=\"max moment error\")\n",
    "            plt.plot(data.get(\"kl\", []), label=\"KL\")\n",
    "            plt.title(\"IIS convergence\")\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if \"w_entropy\" in data:\n",
    "            plt.figure(figsize=(5,3))\n",
    "            plt.plot(data[\"w_entropy\"], label=\"weight entropy\")\n",
    "            plt.title(\"Weight entropy\")\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Quick PMF sanity on the configured backend\n",
    "    backend = create_backend(backend_name=\"vmf_softmax\", n_components=min(3, max(2, CFG[\"components_per_layer\"])), seed=0, vmf_kappa=CFG[\"vmf_kappa\"])\n",
    "    X = np.eye(3, dtype=np.float64)\n",
    "    backend.fit(X)\n",
    "    gamma = backend.predict_proba(X)\n",
    "    print(f\"[Diag] gamma min={gamma.min():.4f}, row sums={gamma.sum(axis=1)}\")\n",
    "\n",
    "    device_for_adapter = getattr(backend, \"device\", None) or torch.device(\"cpu\")\n",
    "    adapter = MaxEntAdapter(\n",
    "        num_classes=2,\n",
    "        layers=[str(l) for l in CFG[\"feature_layers\"][:2]],\n",
    "        components_per_layer={str(l): max(2, CFG[\"components_per_layer\"]) for l in CFG[\"feature_layers\"][:2]},\n",
    "        device=device_for_adapter,\n",
    "    )\n",
    "    resp = {adapter.layers[0]: backend.predict_proba(X), adapter.layers[1]: backend.predict_proba(X)} if len(adapter.layers) > 1 else {adapter.layers[0]: backend.predict_proba(X)}\n",
    "    class_probs = np.full((gamma.shape[0], 2), 0.5)\n",
    "    joint = adapter.joint_builder.build_joint_from_responsibilities({k: np.array(v) for k, v in resp.items()}, class_probs)\n",
    "    flat, mass = adapter.joint_builder.validate_and_flatten({k: adapter.joint_builder.to_tensor(v) for k, v in joint.items()}, rel_mass_tol=1e-6)\n",
    "    print(f\"[Diag] joint feature mass (should track #layers): {mass}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f76abf",
   "metadata": {},
   "source": [
    "## 9) Final results summary\n",
    "Pivot tables (method x seed) plus meanxstd. Sweep top-k shown when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd822728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if RUN_LOG:\n",
    "    results_df = pd.DataFrame(RUN_LOG)\n",
    "    pivot = results_df.pivot_table(index=\"method_label\", columns=\"seed\", values=\"target_acc\")\n",
    "    stats = results_df.groupby(\"method_label\")[\"target_acc\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n",
    "    print(\"Method x Seed target_acc:\")\n",
    "    display(pivot)\n",
    "    print(\"Mean +/- std:\")\n",
    "    display(stats)\n",
    "    summary_csv = RUN_ROOT / \"summary.csv\"\n",
    "    stats.to_csv(summary_csv, index=False)\n",
    "    print(f\"[Summary] Saved: {summary_csv}\")\n",
    "else:\n",
    "    print(\"[Results] RUN_LOG is empty; run baselines/sweeps first.\")\n",
    "\n",
    "sweep_csv = RUN_ROOT / \"vmf_sweep\" / \"vmf_sweep.csv\"\n",
    "if sweep_csv.exists():\n",
    "    sweep_df = pd.read_csv(sweep_csv)\n",
    "    top10 = sweep_df.sort_values(\"target_acc\", ascending=False).head(10)\n",
    "    print(\"Top sweep configs (by target_acc):\")\n",
    "    display(top10)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
