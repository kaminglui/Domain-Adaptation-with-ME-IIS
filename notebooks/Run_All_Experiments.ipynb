{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n5z7DpfPb-M",
        "outputId": "64df56cd-057c-424c-e52a-4db62aa98208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "DRIVE_ROOT: /content/drive/MyDrive/ME-IIS\n",
            "DATA_ROOT: /content/drive/MyDrive/ME-IIS/data\n",
            "LOCAL_DATA_ROOT: /content/data\n",
            "CKPT_ROOT: /content/drive/MyDrive/ME-IIS/checkpoints\n",
            "OUT_ROOT: /content/drive/MyDrive/ME-IIS/results\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# [A1] Drive + Paths (Camelyon17/WILDS)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "else:\n",
        "    print('Not running inside Colab; paths below target /content for reproducibility.')\n",
        "\n",
        "# Select a project folder under MyDrive (edit this once).\n",
        "PROJECT_FOLDER = 'ME-IIS'\n",
        "DRIVE_ROOT = f\"/content/drive/MyDrive/{PROJECT_FOLDER}\"\n",
        "DATA_ROOT  = f\"{DRIVE_ROOT}/data\"          # persistent (Drive)\n",
        "CKPT_ROOT  = f\"{DRIVE_ROOT}/checkpoints\"   # persistent (Drive)\n",
        "OUT_ROOT   = f\"{DRIVE_ROOT}/results\"       # persistent (Drive)\n",
        "LOCAL_SCRATCH = '/content'                  # fast local disk\n",
        "\n",
        "# Prefer LOCAL_SCRATCH for training data I/O.\n",
        "LOCAL_DATA_ROOT = f\"{LOCAL_SCRATCH}/data\"\n",
        "\n",
        "for p in [DATA_ROOT, CKPT_ROOT, OUT_ROOT, LOCAL_DATA_ROOT]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# Pipeline selector for legacy cells.\n",
        "RUN_PIPELINE = 'camelyon17'\n",
        "print('DRIVE_ROOT:', DRIVE_ROOT)\n",
        "print('DATA_ROOT:', DATA_ROOT)\n",
        "print('LOCAL_DATA_ROOT:', LOCAL_DATA_ROOT)\n",
        "print('CKPT_ROOT:', CKPT_ROOT)\n",
        "print('OUT_ROOT:', OUT_ROOT)\n"
      ],
      "id": "1n5z7DpfPb-M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN1Qxyc1Pb-Y"
      },
      "source": [
        "# Camelyon17 (WILDS) Experiments (ME-IIS + UDA baselines)\n",
        "\n",
        "This section adds a clean Camelyon17 pipeline using the official `wilds` library.\n",
        "Legacy Office-Home cells remain below but are skipped by default.\n"
      ],
      "id": "JN1Qxyc1Pb-Y"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCvcTee8Pb-c",
        "outputId": "8f129179-ef3d-44c9-8673-46d5fab4a152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repo...\n",
            "HEAD commit: 447e675b1f40df73d505f4b465234b0a28060069\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# ============================================================\n",
        "# [A2] Repo Checkout (latest main)\n",
        "# ============================================================\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = 'https://github.com/kaminglui/ME-IIS.git'\n",
        "REPO_DIR = Path('/content/ME-IIS')\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    print('Cloning repo...')\n",
        "    subprocess.check_call(['git', 'clone', REPO_URL, str(REPO_DIR)])\n",
        "\n",
        "subprocess.check_call(['git', '-C', str(REPO_DIR), 'fetch', '--all', '--tags'])\n",
        "subprocess.check_call(['git', '-C', str(REPO_DIR), 'checkout', 'main'])\n",
        "subprocess.check_call(['git', '-C', str(REPO_DIR), 'pull', '--ff-only'])\n",
        "\n",
        "GIT_SHA = subprocess.check_output(['git', '-C', str(REPO_DIR), 'rev-parse', 'HEAD'], text=True).strip()\n",
        "print('HEAD commit:', GIT_SHA)\n",
        "\n",
        "import sys\n",
        "if str(REPO_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_DIR))\n",
        "\n",
        "# Persist commit for reproducibility.\n",
        "Path(OUT_ROOT, 'git_commit.txt').write_text(GIT_SHA + '\\n', encoding='utf-8')\n"
      ],
      "id": "rCvcTee8Pb-c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcL83uXUPb-d",
        "outputId": "c14b3f86-579f-4de3-9f4a-4b17cbb7e314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: ['wilds']\n",
            "torch: 2.9.0+cu126\n",
            "torchvision: 0.24.0+cu126\n",
            "wilds version: 2.0.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# [A3] Install Dependencies\n",
        "# ============================================================\n",
        "\n",
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "reqs = [\n",
        "    ('torch', 'torch'),\n",
        "    ('torchvision', 'torchvision'),\n",
        "    ('numpy', 'numpy'),\n",
        "    ('pandas', 'pandas'),\n",
        "    ('tqdm', 'tqdm'),\n",
        "    ('scikit-learn', 'sklearn'),\n",
        "    ('matplotlib', 'matplotlib'),\n",
        "    ('psutil', 'psutil'),\n",
        "    ('wilds', 'wilds'),\n",
        "]\n",
        "\n",
        "def _is_installed(mod: str) -> bool:\n",
        "    try:\n",
        "        importlib.import_module(mod)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "missing = [pkg for pkg, mod in reqs if not _is_installed(mod)]\n",
        "if missing:\n",
        "    print('Installing:', missing)\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *missing])\n",
        "else:\n",
        "    print('All core deps already installed.')\n",
        "\n",
        "import torch, torchvision\n",
        "print('torch:', torch.__version__)\n",
        "print('torchvision:', torchvision.__version__)\n",
        "\n",
        "import wilds  # type: ignore\n",
        "print('wilds version:', getattr(wilds, '__version__', 'unknown'))\n",
        "\n",
        "# NOTE: WILDS `examples/` code is not included in pip installs.\n",
        "# If you need WILDS example algorithms, install from source:\n",
        "#   !git clone https://github.com/p-lambda/wilds.git\n",
        "#   !pip install -e wilds\n"
      ],
      "id": "mcL83uXUPb-d"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSddFfj-Pb-e",
        "outputId": "cdc3986d-e355-4114-ee5f-283b74c01a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "VRAM (GB): 39.55743408203125\n",
            "RAM (GB): 83.47371673583984\n",
            "Disk /content free (GB): 197.13261795043945\n",
            "Suggested batch_size: 256 | grad_accum_steps: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# [A4] Hardware + Performance Defaults\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print('GPU:', props.name)\n",
        "    print('VRAM (GB):', props.total_memory / (1024**3))\n",
        "else:\n",
        "    print('GPU: none')\n",
        "\n",
        "vm = psutil.virtual_memory()\n",
        "print('RAM (GB):', vm.total / (1024**3))\n",
        "\n",
        "try:\n",
        "    du = shutil.disk_usage(LOCAL_SCRATCH)\n",
        "    print('Disk /content free (GB):', du.free / (1024**3))\n",
        "except Exception as exc:\n",
        "    print('Disk usage unavailable:', exc)\n",
        "\n",
        "# Performance toggles\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try:\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
        "PIN_MEMORY = True\n",
        "PERSISTENT_WORKERS = True\n",
        "\n",
        "def suggest_batch_and_accum(vram_gb: float, target_effective_bs: int = 256):\n",
        "    # Very rough heuristic; tune as needed.\n",
        "    if vram_gb >= 38:\n",
        "        bs = 256\n",
        "    elif vram_gb >= 24:\n",
        "        bs = 128\n",
        "    elif vram_gb >= 16:\n",
        "        bs = 64\n",
        "    else:\n",
        "        bs = 32\n",
        "    accum = max(1, target_effective_bs // bs)\n",
        "    return bs, accum\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    bs, accum = suggest_batch_and_accum(torch.cuda.get_device_properties(0).total_memory / (1024**3))\n",
        "    print('Suggested batch_size:', bs, '| grad_accum_steps:', accum)\n"
      ],
      "id": "HSddFfj-Pb-e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmLHvxgQPb-g"
      },
      "source": [
        "# Camelyon17 runs (ERM / DANN / ME-IIS)\n",
        "\n",
        "Defaults:\n",
        "- WILDS-2.0-style UDA: labeled train + unlabeled target (`test_unlabeled`)\n",
        "- Early stopping on OOD val (`val`)\n"
      ],
      "id": "jmLHvxgQPb-g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyjUU3QZQu0C"
      },
      "source": [
        "# Camelyon17 runs (ERM / DANN / ME-IIS)\n",
        "\n",
        "Defaults:\n",
        "- WILDS-2.0-style UDA: labeled train + unlabeled target (`test_unlabeled`)\n",
        "- Early stopping on OOD val (`val`)\n"
      ],
      "id": "iyjUU3QZQu0C"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "bNAmjG6aPb-h",
        "outputId": "4ddbcd1c-86eb-4d9a-e7d3-4d3a502f7938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[hparams] {\"algorithm\": \"ERM\", \"amp\": true, \"augment\": false, \"backbone\": \"densenet121\", \"batch_size\": 64, \"color_jitter\": false, \"dataset\": \"camelyon17\", \"deterministic\": true, \"epochs\": 5, \"grad_accum_steps\": 1, \"lr\": 0.0001, \"optimizer\": \"adamw\", \"paper_match\": false, \"pretrained\": false, \"weight_decay\": 0.0}\n",
            "Downloading dataset to /content/data/wilds/camelyon17_unlabeled_v1.0...\n",
            "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
            "Downloading https://worksheets.codalab.org/rest/bundles/0xa78be8a88a00487a92006936514967d2/contents/blob/ to /content/data/wilds/camelyon17_unlabeled_v1.0/archive.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|‚ñè         | 1537728512/69442379933 [00:48<17:43, 63851100.16Byte/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3959838228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mFORCE_RERUN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_camelyon17_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uda_target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_rerun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFORCE_RERUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m summary_path = run_experiments(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWILDS_DATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ME-IIS/src/run_experiments.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(configs, data_root, ckpt_root, out_root, device, summary_name)\u001b[0m\n\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         loaders = build_camelyon17_loaders(\n\u001b[0m\u001b[1;32m    338\u001b[0m             {\n\u001b[1;32m    339\u001b[0m                 \u001b[0;34m\"data_root\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ME-IIS/src/datasets/wilds_camelyon17.py\u001b[0m in \u001b[0;36mbuild_camelyon17_loaders\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mloader_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loader_kwargs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_camelyon17_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_camelyon17_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ME-IIS/src/datasets/wilds_camelyon17.py\u001b[0m in \u001b[0;36mget_camelyon17_dataset\u001b[0;34m(root_dir, download, unlabeled)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_camelyon17_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mwilds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_require_wilds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"camelyon17\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/get_dataset.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(dataset, version, unlabeled, **dataset_kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munlabeled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlabeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamelyon17_unlabeled_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCamelyon17UnlabeledDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mCamelyon17UnlabeledDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mwilds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamelyon17_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCamelyon17Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/datasets/unlabeled/camelyon17_unlabeled_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, version, root_dir, download, split_scheme)\u001b[0m\n\u001b[1;32m     60\u001b[0m     ):\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_data_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_resolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/datasets/unlabeled/wilds_unlabeled_dataset.py\u001b[0m in \u001b[0;36minitialize_data_dir\u001b[0;34m(self, root_dir, download)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_data_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/datasets/wilds_dataset.py\u001b[0m in \u001b[0;36minitialize_data_dir\u001b[0;34m(self, root_dir, download)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;31m# If the dataset exists at root_dir, then don't download.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_exists_locally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/datasets/wilds_dataset.py\u001b[0m in \u001b[0;36mdownload_dataset\u001b[0;34m(self, data_dir, download_flag)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             download_and_extract_archive(\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/datasets/download_utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished, size)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wilds/datasets/download_utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, size)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             urllib.request.urlretrieve(\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    601\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mchunk_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mIncompleteRead\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdetect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from src.run_experiments import default_camelyon17_configs, run_experiments\n",
        "\n",
        "# Prefer local scratch for the WILDS dataset download.\n",
        "WILDS_DATA_ROOT = f\"{LOCAL_DATA_ROOT}/wilds\"\n",
        "\n",
        "FORCE_RERUN = False\n",
        "configs = default_camelyon17_configs(seed=0, split_mode='uda_target', force_rerun=FORCE_RERUN)\n",
        "summary_path = run_experiments(\n",
        "    configs,\n",
        "    data_root=WILDS_DATA_ROOT,\n",
        "    ckpt_root=CKPT_ROOT,\n",
        "    out_root=OUT_ROOT,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        ")\n",
        "print('Summary CSV:', summary_path)\n"
      ],
      "id": "bNAmjG6aPb-h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjweN7PdPb-j"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "- Bar chart: test OOD accuracy by method\n",
        "- ME-IIS diagnostics: objective vs IIS iteration, ESS vs epoch, weight histogram\n"
      ],
      "id": "OjweN7PdPb-j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ0kU_0kPb-l"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "summary_csv = Path(OUT_ROOT) / 'summary.csv'\n",
        "df = pd.read_csv(summary_csv)\n",
        "print(df)\n",
        "\n",
        "# Bar chart for test accuracy\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "ax.bar(df['algorithm'], df['test_acc'])\n",
        "ax.set_ylabel('OOD test accuracy')\n",
        "ax.set_title('Camelyon17 (WILDS)')\n",
        "plt.show()\n",
        "\n",
        "# ME-IIS diagnostics (best-effort)\n",
        "meiis_rows = df[df['algorithm'].str.upper() == 'MEIIS']\n",
        "if len(meiis_rows) > 0:\n",
        "    run_id = meiis_rows.iloc[0]['run_id']\n",
        "    run_json = Path(OUT_ROOT) / 'runs' / f\"{run_id}.json\"\n",
        "    payload = json.loads(run_json.read_text(encoding='utf-8'))\n",
        "    updates = (payload.get('meiis', {}) or {}).get('weight_updates', [])\n",
        "    ess = [u.get('ess') for u in updates if u.get('status') == 'updated' and u.get('ess') is not None]\n",
        "\n",
        "    if ess:\n",
        "        fig, ax = plt.subplots(figsize=(6, 3))\n",
        "        ax.plot(range(len(ess)), ess)\n",
        "        ax.set_xlabel('epoch')\n",
        "        ax.set_ylabel('ESS')\n",
        "        ax.set_title('ME-IIS effective sample size')\n",
        "        plt.show()\n",
        "\n",
        "    # Objective curve from the latest successful update\n",
        "    obj = None\n",
        "    for u in reversed(updates):\n",
        "        if isinstance(u, dict) and u.get('status') == 'updated' and u.get('iis_objective'):\n",
        "            obj = u.get('iis_objective')\n",
        "            break\n",
        "    if obj:\n",
        "        fig, ax = plt.subplots(figsize=(6, 3))\n",
        "        ax.plot(range(len(obj)), obj)\n",
        "        ax.set_xlabel('IIS iteration')\n",
        "        ax.set_ylabel('dual objective')\n",
        "        ax.set_title('ME-IIS objective vs IIS iteration')\n",
        "        plt.show()\n",
        "\n",
        "    # Weight histogram from best checkpoint\n",
        "    ckpt_path = Path(CKPT_ROOT) / run_id / 'best.pt'\n",
        "    if ckpt_path.exists():\n",
        "        try:\n",
        "            ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
        "        except TypeError:\n",
        "            ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "        w = ckpt.get('algorithm', {}).get('source_weights', None)\n",
        "        if w is not None:\n",
        "            w = torch.as_tensor(w).flatten().numpy()\n",
        "            fig, ax = plt.subplots(figsize=(6, 3))\n",
        "            ax.hist(w, bins=50)\n",
        "            ax.set_title('ME-IIS source weights')\n",
        "            plt.show()\n"
      ],
      "id": "LQ0kU_0kPb-l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr6qDvk0hgD_"
      },
      "source": [
        "# ME-IIS Colab Benchmark (12 Office-Home transfers)\n",
        "\n",
        "* Baseline UDA: `Source Only`, `DANN`, `DAN`, `JAN`, `CDAN`.\n",
        "* Our UDA: `ME-IIS`."
      ],
      "id": "Nr6qDvk0hgD_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYQs0NEphgEE"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import os\n",
        "    import sys\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "\n",
        "    IN_COLAB = \"google.colab\" in sys.modules\n",
        "    if IN_COLAB:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    else:\n",
        "        print(\"Not running inside Colab; skipping Drive mount (paths still target /content).\")\n",
        "\n",
        "    DRIVE_ROOT = Path(\"/content/drive/MyDrive/ME-IIS\")\n",
        "    DATASETS_DRIVE = DRIVE_ROOT / \"datasets\"\n",
        "    RUNS_DRIVE = DRIVE_ROOT / \"runs\"\n",
        "    WORKDIR = Path(\"/content/work\")\n",
        "\n",
        "    for path in [DRIVE_ROOT, DATASETS_DRIVE, RUNS_DRIVE, WORKDIR]:\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "    def _format_bytes(num):\n",
        "        try:\n",
        "            if num is None:\n",
        "                return \"unknown\"\n",
        "            num = float(num)\n",
        "            for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n",
        "                if num < 1024 or unit == \"PB\":\n",
        "                    return f\"{num:.2f}{unit}\"\n",
        "                num /= 1024.0\n",
        "        except Exception:\n",
        "            return str(num)\n",
        "\n",
        "\n",
        "    def _print_disk(path, label):\n",
        "        try:\n",
        "            total, used, free = shutil.disk_usage(str(path))\n",
        "            print(f\"{label}: total={_format_bytes(total)} free={_format_bytes(free)} path={path}\")\n",
        "        except Exception as exc:\n",
        "            print(f\"{label}: unable to read disk usage ({exc})\")\n",
        "\n",
        "\n",
        "    _print_disk(\"/content\", \"Local /content disk\")\n",
        "    if DRIVE_ROOT.exists():\n",
        "        _print_disk(DRIVE_ROOT, \"Drive disk\")\n",
        "    else:\n",
        "        print(\"Drive path missing (mount may have been skipped).\")\n",
        "\n",
        "\n",
        "    def print_system_info():\n",
        "        gpu_msg = \"GPU: none detected\"\n",
        "        try:\n",
        "            import torch\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                props = torch.cuda.get_device_properties(0)\n",
        "                free, total = torch.cuda.mem_get_info()\n",
        "                gpu_msg = f\"GPU: {props.name} free={_format_bytes(free)} total={_format_bytes(total)}\"\n",
        "            else:\n",
        "                gpu_msg = \"GPU: CPU-only runtime\"\n",
        "        except Exception as exc:\n",
        "            gpu_msg = f\"GPU check failed: {exc}\"\n",
        "        print(gpu_msg)\n",
        "\n",
        "        cpu = os.cpu_count()\n",
        "        print(f\"CPU cores: {cpu}\")\n",
        "        try:\n",
        "            import psutil  # type: ignore\n",
        "\n",
        "            vm = psutil.virtual_memory()\n",
        "            print(f\"RAM: total={_format_bytes(vm.total)} available={_format_bytes(vm.available)}\")\n",
        "        except Exception:\n",
        "            print(\"RAM: psutil not available\")\n",
        "        _print_disk(\"/content\", \"Local disk snapshot\")\n",
        "\n",
        "\n",
        "    print_system_info()\n",
        "    print(f\"DRIVE_ROOT={DRIVE_ROOT}\")\n",
        "    print(\"Avoid saving checkpoints directly to Drive during training; final artifacts are copied after runs.\")\n",
        "\n"
      ],
      "id": "vYQs0NEphgEE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqUyqSOKhgEH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import subprocess\n",
        "    from pathlib import Path\n",
        "\n",
        "    REPO_URL = \"https://github.com/kaminglui/ME-IIS.git\"\n",
        "    REPO_DIR = Path(\"/content/ME-IIS\")\n",
        "\n",
        "    if not REPO_DIR.exists():\n",
        "        print(\"Cloning repo...\")\n",
        "        subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)])\n",
        "    else:\n",
        "        print(\"Repo already present, fetching latest main...\")\n",
        "        subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"fetch\", \"--all\", \"--tags\"])\n",
        "        subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"checkout\", \"main\"])\n",
        "        subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\", \"--ff-only\"])\n",
        "\n",
        "    sha = (\n",
        "        subprocess.check_output([\"git\", \"-C\", str(REPO_DIR), \"rev-parse\", \"HEAD\"], text=True)\n",
        "        .strip()\n",
        "    )\n",
        "    status = subprocess.check_output([\"git\", \"-C\", str(REPO_DIR), \"status\", \"-sb\"], text=True).strip()\n",
        "\n",
        "    print(f\"HEAD commit: {sha}\")\n",
        "    print(\"Git status:\")\n",
        "    print(status)\n",
        "\n",
        "    import sys\n",
        "\n",
        "    if str(REPO_DIR) not in sys.path:\n",
        "        sys.path.insert(0, str(REPO_DIR))\n",
        "\n"
      ],
      "id": "EqUyqSOKhgEH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZb-lNnhgEI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import importlib\n",
        "    import subprocess\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    REQ_FILE = Path(REPO_DIR) / \"requirements.txt\"\n",
        "\n",
        "\n",
        "    def _is_installed(mod_name: str) -> bool:\n",
        "        try:\n",
        "            importlib.import_module(mod_name)\n",
        "            return True\n",
        "        except ImportError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    missing_core = [m for m in [\"torch\", \"torchvision\", \"sklearn\"] if not _is_installed(m)]\n",
        "    if missing_core:\n",
        "        print(f\"Installing requirements (missing: {missing_core})...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(REQ_FILE)])\n",
        "    else:\n",
        "        print(\"Core packages already installed; skipping requirements install.\")\n",
        "\n",
        "    if not _is_installed(\"kagglehub\"):\n",
        "        print(\"Installing kagglehub for dataset downloads...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kagglehub\"])\n",
        "    else:\n",
        "        print(\"kagglehub already installed.\")\n",
        "\n"
      ],
      "id": "rMZb-lNnhgEI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQVoku1hgEI"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import os\n",
        "    import json\n",
        "    import shutil\n",
        "    import zipfile\n",
        "    from pathlib import Path\n",
        "    from typing import Any, Dict\n",
        "\n",
        "    import kagglehub\n",
        "\n",
        "    OFFICE_HOME_DOMAIN_SETS = [\n",
        "        [\"Art\", \"Clipart\", \"Product\", \"RealWorld\"],\n",
        "        [\"Art\", \"Clipart\", \"Product\", \"Real World\"],\n",
        "        [\"Art\", \"Clipart\", \"Product\", \"Real_World\"],\n",
        "        [\"Art\", \"Clipart\", \"Product\", \"Real\"],\n",
        "    ]\n",
        "    OFFICE31_DOMAINS = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "\n",
        "\n",
        "    def _maybe_extract_zip(path: Path) -> Path:\n",
        "        if path.is_file() and path.suffix.lower() == \".zip\":\n",
        "            target = path.with_suffix(\"\")\n",
        "            target.mkdir(parents=True, exist_ok=True)\n",
        "            marker = target / \".extracted_ok\"\n",
        "            if not marker.exists():\n",
        "                print(f\"[DATA] Extracting {path} -> {target}\")\n",
        "                with zipfile.ZipFile(path, \"r\") as zf:\n",
        "                    zf.extractall(target)\n",
        "                marker.write_text(\"ok\", encoding=\"utf-8\")\n",
        "            return target\n",
        "        return path\n",
        "\n",
        "\n",
        "    def _find_office_home_root(raw: Path) -> Path:\n",
        "        raw = _maybe_extract_zip(Path(raw))\n",
        "        candidates = [raw] + [p for p in raw.rglob(\"*\") if p.is_dir()]\n",
        "        for candidate in candidates:\n",
        "            for domains in OFFICE_HOME_DOMAIN_SETS:\n",
        "                if all((candidate / d).exists() for d in domains):\n",
        "                    return candidate\n",
        "        raise FileNotFoundError(f\"Could not locate Office-Home domains under {raw}\")\n",
        "\n",
        "\n",
        "    def _find_office31_root(raw: Path) -> Path:\n",
        "        raw = _maybe_extract_zip(Path(raw))\n",
        "        candidates = [raw] + [p for p in raw.rglob(\"*\") if p.is_dir()]\n",
        "        for candidate in candidates:\n",
        "            if all((candidate / d).exists() for d in OFFICE31_DOMAINS):\n",
        "                return candidate\n",
        "        raise FileNotFoundError(f\"Could not locate Office-31 domains under {raw}\")\n",
        "\n",
        "\n",
        "    def resolve_dataset_root(dataset_name: str) -> Path:\n",
        "        if dataset_name == \"office_home\":\n",
        "            raw = Path(kagglehub.dataset_download(\"lhrrraname/officehome\"))\n",
        "            return _find_office_home_root(raw)\n",
        "        if dataset_name == \"office31\":\n",
        "            raw = Path(kagglehub.dataset_download(\"xixuhu/office31\"))\n",
        "            return _find_office31_root(raw)\n",
        "        raise ValueError(f\"Unknown DATASET_NAME={dataset_name}\")\n",
        "\n",
        "\n",
        "    def _dir_size_bytes(path: Path) -> int:\n",
        "        total = 0\n",
        "        for fp in path.rglob(\"*\"):\n",
        "            if fp.is_file():\n",
        "                try:\n",
        "                    total += int(fp.stat().st_size)\n",
        "                except Exception:\n",
        "                    continue\n",
        "        return total\n",
        "\n",
        "\n",
        "    def ensure_dataset_in_drive(dataset_name: str, datasets_drive_dir: Path) -> Path:\n",
        "        datasets_drive_dir = Path(datasets_drive_dir)\n",
        "        datasets_drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "        drive_path = datasets_drive_dir / dataset_name\n",
        "        if drive_path.exists():\n",
        "            print(f\"[DATA] Using cached dataset in Drive: {drive_path}\")\n",
        "            return drive_path\n",
        "        raw_root = resolve_dataset_root(dataset_name)\n",
        "        drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"[DATA] Copying dataset into Drive cache: {raw_root} -> {drive_path}\")\n",
        "        shutil.copytree(raw_root, drive_path, dirs_exist_ok=True)\n",
        "        return drive_path\n",
        "\n",
        "\n",
        "    def maybe_copy_dataset_to_local(drive_path: Path, local_path: Path, enable: bool = True, headroom_gb: int = 5) -> Path:\n",
        "        drive_path = Path(drive_path)\n",
        "        local_path = Path(local_path)\n",
        "        if not enable:\n",
        "            print(f\"[DATA] Local caching disabled; using Drive path: {drive_path}\")\n",
        "            return drive_path\n",
        "        marker = local_path / \".cached_ok\"\n",
        "        if marker.exists():\n",
        "            print(f\"[DATA] Using existing local copy: {local_path}\")\n",
        "            return local_path\n",
        "        try:\n",
        "            free = shutil.disk_usage(str(local_path.parent)).free\n",
        "        except Exception:\n",
        "            free = None\n",
        "        dataset_size = _dir_size_bytes(drive_path)\n",
        "        if free is not None and dataset_size > max(0, free - headroom_gb * 1024**3):\n",
        "            print(f\"[DATA] Insufficient local disk; using Drive path ({drive_path})\")\n",
        "            return drive_path\n",
        "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"[DATA] Copying dataset from Drive to local SSD: {drive_path} -> {local_path}\")\n",
        "        shutil.copytree(drive_path, local_path, dirs_exist_ok=True)\n",
        "        marker.write_text(\"ok\", encoding=\"utf-8\")\n",
        "        return local_path\n",
        "\n",
        "\n",
        "    print(\"Dataset helpers ready. Use ensure_dataset_in_drive(...) then maybe_copy_dataset_to_local(...).\")\n",
        "\n"
      ],
      "id": "bYQVoku1hgEI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXIhx8OThgEK"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    DATASETS_LOCAL = WORKDIR / \"datasets\"\n",
        "    RUNS_LOCAL = WORKDIR / \"runs\"\n",
        "    RUNS_LOCAL_INTERNAL = WORKDIR / \"runs_internal\"\n",
        "    for path in [DATASETS_LOCAL, RUNS_LOCAL, RUNS_LOCAL_INTERNAL]:\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    DATASET_NAME = \"office_home\"\n",
        "    DOMAINS = [\"Ar\", \"Cl\", \"Pr\", \"Rw\"] if DATASET_NAME == \"office_home\" else [\"A\", \"D\", \"W\"]\n",
        "    METHODS = [\"source_only\", \"dann\", \"dan\", \"jan\", \"cdan\", \"me_iis\"]  # CORAL intentionally excluded\n",
        "    SEEDS = [0]  # for full benchmark: [0, 1, 2]\n",
        "\n",
        "    EPOCHS_SOURCE = 50\n",
        "    EPOCHS_ADAPT = 10\n",
        "    BATCH_SIZE = 32  # set to \"auto\" to probe largest fitting batch\n",
        "    NUM_WORKERS = \"auto\"  # auto-tuned below\n",
        "    AMP = True\n",
        "    DETERMINISTIC = False\n",
        "    FORCE_RERUN = False\n",
        "    SAVE_EVERY_EPOCH = False\n",
        "\n",
        "    BACKBONE = \"resnet50\"\n",
        "    INPUT_SIZE = 224\n",
        "\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    RUN_TAG = f\"{DATASET_NAME}_full_{timestamp}\"\n",
        "\n",
        "    SOURCE_ONLY_PARAMS = {\"amp\": AMP}\n",
        "\n",
        "    DANN_PARAMS = {\n",
        "        \"amp\": AMP,\n",
        "        \"disc_hidden_dim\": 1024,\n",
        "        \"disc_dropout\": 0.1,\n",
        "        \"grl_lambda_schedule\": \"grl\",\n",
        "        \"domain_loss_weight\": 1.0,\n",
        "    }\n",
        "\n",
        "    DAN_PARAMS = {\n",
        "        \"amp\": AMP,\n",
        "        \"mmd_loss_weight\": 1.0,\n",
        "        \"mmd_kernel_mul\": 2.0,\n",
        "        \"mmd_kernel_num\": 5,\n",
        "        \"mmd_fix_sigma\": None,\n",
        "    }\n",
        "\n",
        "    JAN_PARAMS = {\n",
        "        \"amp\": AMP,\n",
        "        \"jmmd_loss_weight\": 1.0,\n",
        "        \"jmmd_kernel_mul\": 2.0,\n",
        "        \"jmmd_kernel_num\": 5,\n",
        "        \"jmmd_fix_sigma\": None,\n",
        "    }\n",
        "\n",
        "    CDAN_PARAMS = {\n",
        "        \"amp\": AMP,\n",
        "        \"disc_hidden_dim\": 1024,\n",
        "        \"disc_dropout\": 0.1,\n",
        "        \"domain_loss_weight\": 1.0,\n",
        "        \"grl_lambda_schedule\": \"grl\",\n",
        "        \"entropy_conditioning\": False,\n",
        "    }\n",
        "\n",
        "    ME_IIS_PARAMS = {\n",
        "        \"amp\": AMP,\n",
        "        \"feature_layers\": [\"layer3\", \"layer4\"],\n",
        "        \"num_latent_styles\": 5,\n",
        "        \"gmm_bic_min_components\": 2,\n",
        "        \"gmm_bic_max_components\": 8,\n",
        "        \"gmm_reg_covar\": 1e-6,\n",
        "        \"gmm_selection_mode\": \"bic\",\n",
        "        \"iis_iters\": 15,\n",
        "        \"iis_tol\": 1e-3,\n",
        "        \"cluster_backend\": \"gmm\",\n",
        "        \"cluster_clean_ratio\": 1.0,\n",
        "        \"kmeans_n_init\": 10,\n",
        "        \"source_prob_mode\": \"softmax\",\n",
        "        \"use_pseudo_labels\": False,\n",
        "        \"pseudo_conf_thresh\": 0.9,\n",
        "        \"pseudo_max_ratio\": 1.0,\n",
        "        \"pseudo_loss_weight\": 1.0,\n",
        "        \"weight_clip_max\": None,\n",
        "        \"weight_mix_alpha\": 0.0,\n",
        "    }\n",
        "\n",
        "    METHOD_PARAMS = {\n",
        "        \"source_only\": SOURCE_ONLY_PARAMS,\n",
        "        \"dann\": DANN_PARAMS,\n",
        "        \"dan\": DAN_PARAMS,\n",
        "        \"jan\": JAN_PARAMS,\n",
        "        \"cdan\": CDAN_PARAMS,\n",
        "        \"me_iis\": ME_IIS_PARAMS,\n",
        "    }\n",
        "\n",
        "    os.environ.setdefault(\"ME_IIS_AUTO_RESOURCES\", \"1\")\n",
        "    print(f\"Config ready | RUN_TAG={RUN_TAG} | METHODS={METHODS} | DOMAINS={DOMAINS}\")\n",
        "\n"
      ],
      "id": "lXIhx8OThgEK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE-6flSwhgEL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import re\n",
        "    from pathlib import Path\n",
        "    from typing import Any, Dict\n",
        "\n",
        "\n",
        "    def _layer_tag(params: Dict[str, Any]) -> str:\n",
        "        layers = params.get(\"feature_layers\", [\"layer3\", \"layer4\"])\n",
        "        if isinstance(layers, str):\n",
        "            layers = [p.strip() for p in layers.split(\",\") if p.strip()]\n",
        "        return \"-\".join(layers)\n",
        "\n",
        "\n",
        "    def make_run_dir(method: str, dataset: str, src: str, tgt: str, seed: int, base: Path | None = None) -> Path:\n",
        "        base = Path(base or RUNS_LOCAL)\n",
        "        return base / str(RUN_TAG) / str(dataset) / f\"{src}2{tgt}\" / str(method) / f\"seed{seed}\"\n",
        "\n",
        "\n",
        "    def make_ckpt_name(\n",
        "        method: str,\n",
        "        dataset: str,\n",
        "        src: str,\n",
        "        tgt: str,\n",
        "        seed: int,\n",
        "        *,\n",
        "        batch_size: Any | None = None,\n",
        "        epochs_source: Any | None = None,\n",
        "        epochs_adapt: Any | None = None,\n",
        "        method_params: Dict[str, Any] | None = None,\n",
        "    ) -> str:\n",
        "        bs_tag = batch_size if batch_size is not None else BATCH_SIZE\n",
        "        es_tag = epochs_source if epochs_source is not None else EPOCHS_SOURCE\n",
        "        ea_tag = epochs_adapt if epochs_adapt is not None else EPOCHS_ADAPT\n",
        "        name = f\"{method}__{dataset}__{src}2{tgt}__seed{seed}__Esrc{es_tag}__Ead{ea_tag}__bs{bs_tag}\"\n",
        "        if method == \"me_iis\":\n",
        "            params = method_params or METHOD_PARAMS.get(\"me_iis\", {})\n",
        "            layer_tag = _layer_tag(params)\n",
        "            gmm_min = params.get(\"gmm_bic_min_components\", 2)\n",
        "            gmm_max = params.get(\"gmm_bic_max_components\", 8)\n",
        "            iis_iters = params.get(\"iis_iters\", 15)\n",
        "            iis_tol = params.get(\"iis_tol\", 1e-3)\n",
        "            name += f\"__layers{layer_tag}__gmm{gmm_min}-{gmm_max}__iis{iis_iters}-{iis_tol}\"\n",
        "        return name + \".pt\"\n",
        "\n",
        "\n",
        "    def decode_ckpt_name(name: str) -> Dict[str, Any]:\n",
        "        stem = Path(name).name.replace(\".pt\", \"\")\n",
        "        parts = stem.split(\"__\")\n",
        "        info: Dict[str, Any] = {\"raw\": stem}\n",
        "        try:\n",
        "            method, dataset, pair, seed_part, esrc_part, ead_part, bs_part, *rest = parts\n",
        "            src, tgt = pair.split(\"2\", 1)\n",
        "            info.update(\n",
        "                {\n",
        "                    \"method\": method,\n",
        "                    \"dataset\": dataset,\n",
        "                    \"src\": src,\n",
        "                    \"tgt\": tgt,\n",
        "                    \"seed\": int(seed_part.replace(\"seed\", \"\")),\n",
        "                    \"epochs_source\": esrc_part.replace(\"Esrc\", \"\"),\n",
        "                    \"epochs_adapt\": ead_part.replace(\"Ead\", \"\"),\n",
        "                    \"batch_size\": bs_part.replace(\"bs\", \"\"),\n",
        "                }\n",
        "            )\n",
        "            for token in rest:\n",
        "                if token.startswith(\"layers\"):\n",
        "                    info[\"layers\"] = token.replace(\"layers\", \"\")\n",
        "                elif token.startswith(\"gmm\"):\n",
        "                    rng = token.replace(\"gmm\", \"\")\n",
        "                    if \"-\" in rng:\n",
        "                        lo, hi = rng.split(\"-\", 1)\n",
        "                        info[\"gmm_range\"] = (int(lo), int(hi))\n",
        "                elif token.startswith(\"iis\"):\n",
        "                    payload = token.replace(\"iis\", \"\")\n",
        "                    if \"-\" in payload:\n",
        "                        iters, tol = payload.split(\"-\", 1)\n",
        "                        info[\"iis_iters\"] = int(float(iters))\n",
        "                        info[\"iis_tol\"] = float(tol)\n",
        "        except Exception:\n",
        "            info[\"error\"] = \"unable to decode\"\n",
        "        return info\n",
        "\n"
      ],
      "id": "lE-6flSwhgEL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uovVkWxhgEL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import math\n",
        "    import random\n",
        "    import os\n",
        "    from typing import Dict, Any\n",
        "\n",
        "    import torch\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "    def set_global_seed(seed: int) -> None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = bool(DETERMINISTIC)\n",
        "        torch.backends.cudnn.benchmark = not bool(DETERMINISTIC)\n",
        "\n",
        "\n",
        "    def auto_dataloader_params(requested: Any = NUM_WORKERS) -> Dict[str, Any]:\n",
        "        cpu = os.cpu_count() or 2\n",
        "        num_workers = requested\n",
        "        if requested == \"auto\":\n",
        "            num_workers = min(8, max(2, cpu - 1))\n",
        "            try:\n",
        "                import psutil  # type: ignore\n",
        "\n",
        "                available = psutil.virtual_memory().available\n",
        "                if available < 8 * 1024**3:\n",
        "                    num_workers = max(0, min(num_workers, 2))\n",
        "            except Exception:\n",
        "                pass\n",
        "        try:\n",
        "            num_workers = int(num_workers)\n",
        "        except Exception:\n",
        "            num_workers = 0\n",
        "        if num_workers < 0:\n",
        "            num_workers = 0\n",
        "        pin_memory = torch.cuda.is_available()\n",
        "        prefetch = 2 if num_workers > 0 else None\n",
        "        persistent_workers = bool(num_workers > 0)\n",
        "        return {\n",
        "            \"num_workers\": num_workers,\n",
        "            \"pin_memory\": pin_memory,\n",
        "            \"prefetch_factor\": prefetch,\n",
        "            \"persistent_workers\": persistent_workers,\n",
        "        }\n",
        "\n",
        "\n",
        "    def auto_batch_size(candidates=(64, 32, 16), device: str | None = None) -> int:\n",
        "        if BATCH_SIZE != \"auto\":\n",
        "            return int(BATCH_SIZE)\n",
        "        device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        for bs in candidates:\n",
        "            try:\n",
        "                _ = torch.zeros((bs, 3, INPUT_SIZE, INPUT_SIZE), device=device)\n",
        "                del _\n",
        "                return int(bs)\n",
        "            except RuntimeError as exc:\n",
        "                if \"out of memory\" in str(exc).lower():\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    continue\n",
        "                raise\n",
        "        return int(candidates[-1])\n",
        "\n",
        "\n",
        "    AUTO_DL = auto_dataloader_params(NUM_WORKERS)\n",
        "    AUTO_BATCH = auto_batch_size()\n",
        "    print(f\"Auto dataloader params: {AUTO_DL}\")\n",
        "    print(f\"Batch size (requested={BATCH_SIZE}) resolved to: {AUTO_BATCH}\")\n",
        "\n"
      ],
      "id": "8uovVkWxhgEL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NSlUjCnhgEM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import csv\n",
        "    import json\n",
        "    import shutil\n",
        "    import traceback\n",
        "    from pathlib import Path\n",
        "    from typing import Dict, Any\n",
        "\n",
        "    from src.experiments.run_config import RunConfig, get_run_dir\n",
        "    from src.experiments.runner import run_one\n",
        "\n",
        "\n",
        "    def _read_metrics_csv(path: Path) -> tuple[float | None, float | None, Dict[str, Any] | None]:\n",
        "        if not path.exists():\n",
        "            return None, None, None\n",
        "        try:\n",
        "            with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                rows = list(reader)\n",
        "        except Exception:\n",
        "            return None, None, None\n",
        "        if not rows:\n",
        "            return None, None, None\n",
        "        row = rows[-1]\n",
        "        try:\n",
        "            src_acc = float(row.get(\"source_acc\", \"nan\"))\n",
        "        except Exception:\n",
        "            src_acc = None\n",
        "        try:\n",
        "            tgt_acc = float(row.get(\"target_acc\", \"nan\"))\n",
        "        except Exception:\n",
        "            tgt_acc = None\n",
        "        return src_acc, tgt_acc, row\n",
        "\n",
        "\n",
        "    def _safe_copy(src: Path, dst: Path) -> None:\n",
        "        if src.exists():\n",
        "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(src, dst)\n",
        "\n",
        "\n",
        "    def run_experiment(\n",
        "        method: str,\n",
        "        src: str,\n",
        "        tgt: str,\n",
        "        seed: int,\n",
        "        data_root: Path,\n",
        "        runs_dir_local: Path,\n",
        "        runs_dir_drive: Path,\n",
        "    ) -> Dict[str, Any]:\n",
        "        method_params = dict(METHOD_PARAMS.get(method, {}))\n",
        "        method_params.setdefault(\"amp\", AMP)\n",
        "        resolved_batch_size = auto_batch_size()\n",
        "        dl_params = auto_dataloader_params(NUM_WORKERS)\n",
        "        resolved_num_workers = dl_params[\"num_workers\"]\n",
        "\n",
        "        run_dir_local = make_run_dir(method, DATASET_NAME, src, tgt, seed, base=runs_dir_local)\n",
        "        run_dir_drive = make_run_dir(method, DATASET_NAME, src, tgt, seed, base=runs_dir_drive)\n",
        "        run_dir_local.mkdir(parents=True, exist_ok=True)\n",
        "        run_dir_drive.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        ckpt_name = make_ckpt_name(\n",
        "            method,\n",
        "            DATASET_NAME,\n",
        "            src,\n",
        "            tgt,\n",
        "            seed,\n",
        "            batch_size=resolved_batch_size,\n",
        "            method_params=method_params,\n",
        "        )\n",
        "        metrics_drive_path = run_dir_drive / \"metrics.csv\"\n",
        "        ckpt_drive_path = run_dir_drive / ckpt_name\n",
        "\n",
        "        config_payload = {\n",
        "            \"run_tag\": RUN_TAG,\n",
        "            \"dataset\": DATASET_NAME,\n",
        "            \"src\": src,\n",
        "            \"tgt\": tgt,\n",
        "            \"seed\": seed,\n",
        "            \"method\": method,\n",
        "            \"epochs_source\": EPOCHS_SOURCE,\n",
        "            \"epochs_adapt\": EPOCHS_ADAPT,\n",
        "            \"batch_size\": resolved_batch_size,\n",
        "            \"num_workers\": resolved_num_workers,\n",
        "            \"method_params\": method_params,\n",
        "            \"amp\": AMP,\n",
        "            \"deterministic\": DETERMINISTIC,\n",
        "            \"data_root\": str(data_root),\n",
        "            \"run_dir_local\": str(run_dir_local),\n",
        "            \"run_dir_drive\": str(run_dir_drive),\n",
        "        }\n",
        "        for dest in [run_dir_local, run_dir_drive]:\n",
        "            (Path(dest) / \"config.json\").write_text(json.dumps(config_payload, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "        if metrics_drive_path.exists() and ckpt_drive_path.exists() and not FORCE_RERUN:\n",
        "            src_acc, tgt_acc, row = _read_metrics_csv(metrics_drive_path)\n",
        "            print(f\"[SKIP] found existing artifacts for {method} {src}->{tgt} seed={seed}\")\n",
        "            return {\n",
        "                \"status\": \"skipped\",\n",
        "                \"method\": method,\n",
        "                \"src\": src,\n",
        "                \"tgt\": tgt,\n",
        "                \"seed\": seed,\n",
        "                \"source_acc\": src_acc,\n",
        "                \"target_acc\": tgt_acc,\n",
        "                \"metrics_csv\": str(metrics_drive_path),\n",
        "                \"checkpoint\": str(ckpt_drive_path),\n",
        "                \"run_dir\": str(run_dir_drive),\n",
        "                \"config\": config_payload,\n",
        "            }\n",
        "\n",
        "        print(f\"[RUN] method={method} src={src} tgt={tgt} seed={seed}\")\n",
        "        cfg = RunConfig(\n",
        "            dataset_name=DATASET_NAME,\n",
        "            data_root=str(data_root),\n",
        "            source_domain=src,\n",
        "            target_domain=tgt,\n",
        "            method=method,\n",
        "            backbone=BACKBONE,\n",
        "            input_size=INPUT_SIZE,\n",
        "            epochs_source=EPOCHS_SOURCE,\n",
        "            epochs_adapt=EPOCHS_ADAPT,\n",
        "            batch_size=int(resolved_batch_size),\n",
        "            num_workers=int(resolved_num_workers),\n",
        "            method_params=method_params,\n",
        "            seed=int(seed),\n",
        "            deterministic=bool(DETERMINISTIC),\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            result = run_one(\n",
        "                cfg,\n",
        "                force_rerun=FORCE_RERUN,\n",
        "                runs_root=RUNS_LOCAL_INTERNAL,\n",
        "                write_metrics=True,\n",
        "                raise_on_error=False,\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            err = traceback.format_exc()\n",
        "            err_path = run_dir_drive / \"stderr.txt\"\n",
        "            err_path.write_text(err, encoding=\"utf-8\")\n",
        "            return {\n",
        "                \"status\": \"failed\",\n",
        "                \"method\": method,\n",
        "                \"src\": src,\n",
        "                \"tgt\": tgt,\n",
        "                \"seed\": seed,\n",
        "                \"error\": str(exc),\n",
        "                \"stderr\": str(err_path),\n",
        "                \"run_dir\": str(run_dir_drive),\n",
        "            }\n",
        "\n",
        "        internal_run_dir = get_run_dir(cfg, runs_root=RUNS_LOCAL_INTERNAL)\n",
        "        metrics_internal = internal_run_dir / \"metrics.csv\"\n",
        "        logs_internal = internal_run_dir / \"logs\"\n",
        "        ckpt_internal = None\n",
        "        if isinstance(result, dict) and \"checkpoint\" in result:\n",
        "            ckpt_internal = Path(result.get(\"checkpoint\")) if result.get(\"checkpoint\") else None\n",
        "        if ckpt_internal is None or not ckpt_internal.exists():\n",
        "            for cand in (internal_run_dir / \"checkpoints\").glob(\"*final*.pth\"):\n",
        "                ckpt_internal = cand\n",
        "                break\n",
        "\n",
        "        _safe_copy(metrics_internal, run_dir_local / \"metrics.csv\")\n",
        "        _safe_copy(metrics_internal, metrics_drive_path)\n",
        "        if logs_internal.exists():\n",
        "            shutil.copytree(logs_internal, run_dir_local / \"logs\", dirs_exist_ok=True)\n",
        "            shutil.copytree(logs_internal, run_dir_drive / \"logs\", dirs_exist_ok=True)\n",
        "        if ckpt_internal is not None and ckpt_internal.exists():\n",
        "            _safe_copy(ckpt_internal, run_dir_local / ckpt_name)\n",
        "            _safe_copy(ckpt_internal, ckpt_drive_path)\n",
        "\n",
        "        src_acc, tgt_acc, row = _read_metrics_csv(run_dir_local / \"metrics.csv\")\n",
        "        status = result.get(\"status\", \"done\") if isinstance(result, dict) else \"done\"\n",
        "        print(f\"[DONE] method={method} src={src} tgt={tgt} seed={seed} target_acc={tgt_acc}\")\n",
        "        return {\n",
        "            \"status\": status,\n",
        "            \"method\": method,\n",
        "            \"src\": src,\n",
        "            \"tgt\": tgt,\n",
        "            \"seed\": seed,\n",
        "            \"source_acc\": src_acc,\n",
        "            \"target_acc\": tgt_acc,\n",
        "            \"metrics_csv\": str(run_dir_local / \"metrics.csv\"),\n",
        "            \"checkpoint\": str(ckpt_drive_path if ckpt_drive_path.exists() else ckpt_internal or \"\"),\n",
        "            \"run_dir\": str(run_dir_drive),\n",
        "            \"config\": config_payload,\n",
        "            \"logs\": str(run_dir_drive / \"logs\"),\n",
        "        }\n",
        "\n"
      ],
      "id": "2NSlUjCnhgEM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "U4JofiFEiOcZ"
      },
      "id": "U4JofiFEiOcZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dStORBkShgEO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    from itertools import product\n",
        "\n",
        "    transfer_pairs = [(s, t) for s in DOMAINS for t in DOMAINS if s != t]\n",
        "    expected_runs = len(transfer_pairs) * len(SEEDS) * len(METHODS)\n",
        "\n",
        "    # Dataset bootstrap: ensure Drive cache -> optional local SSD copy\n",
        "    DATA_ROOT_DRIVE = ensure_dataset_in_drive(DATASET_NAME, DATASETS_DRIVE)\n",
        "    DATA_ROOT_LOCAL = maybe_copy_dataset_to_local(DATA_ROOT_DRIVE, DATASETS_LOCAL / DATASET_NAME, enable=True)\n",
        "    print(f\"[DATA] Training will read from: {DATA_ROOT_LOCAL}\")\n",
        "\n",
        "    results: list[dict] = []\n",
        "    for src, tgt in transfer_pairs:\n",
        "        for seed in SEEDS:\n",
        "            base_res = run_experiment(\"source_only\", src, tgt, seed, DATA_ROOT_LOCAL, RUNS_LOCAL, RUNS_DRIVE)\n",
        "            results.append(base_res)\n",
        "            for method in METHODS:\n",
        "                if method == \"source_only\":\n",
        "                    continue\n",
        "                res = run_experiment(method, src, tgt, seed, DATA_ROOT_LOCAL, RUNS_LOCAL, RUNS_DRIVE)\n",
        "                results.append(res)\n",
        "\n",
        "    print(f\"Finished benchmark loop: {len(results)} runs (expected {expected_runs}).\")\n",
        "\n"
      ],
      "id": "dStORBkShgEO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhupEFethgEP"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import shutil\n",
        "    import pandas as pd\n",
        "    from pathlib import Path\n",
        "    from IPython.display import display\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df[\"transfer\"] = results_df.apply(lambda r: f\"{r.get('src','?')}->{r.get('tgt','?')}\", axis=1)\n",
        "\n",
        "    SUMMARY_LOCAL_PATH = WORKDIR / f\"summary_{RUN_TAG}.csv\"\n",
        "    SUMMARY_DRIVE_PATH = RUNS_DRIVE / f\"summary_{RUN_TAG}.csv\"\n",
        "    results_df.to_csv(SUMMARY_LOCAL_PATH, index=False)\n",
        "    SUMMARY_DRIVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy2(SUMMARY_LOCAL_PATH, SUMMARY_DRIVE_PATH)\n",
        "    print(f\"Saved summary CSV to {SUMMARY_LOCAL_PATH} and {SUMMARY_DRIVE_PATH}\")\n",
        "\n",
        "    valid = results_df[results_df[\"target_acc\"].notnull()]\n",
        "    transfer_stats = (\n",
        "        valid.groupby([\"method\", \"transfer\"])\n",
        "        .agg(target_mean=(\"target_acc\", \"mean\"), target_std=(\"target_acc\", \"std\"))\n",
        "        .reset_index()\n",
        "    )\n",
        "    overall_stats = (\n",
        "        valid.groupby(\"method\")\n",
        "        .agg(target_mean=(\"target_acc\", \"mean\"), target_std=(\"target_acc\", \"std\"))\n",
        "        .reset_index()\n",
        "    )\n",
        "    overall_stats[\"transfer\"] = \"Office-Home Avg\"\n",
        "\n",
        "    print(\"Per-transfer stats (head):\")\n",
        "    display(transfer_stats.head())\n",
        "    print(\"Overall method means:\")\n",
        "    display(overall_stats)\n",
        "\n"
      ],
      "id": "lhupEFethgEP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEWUdCMehgEP"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Bar chart: average target accuracy per method\n",
        "    x_pos = np.arange(len(overall_stats[\"method\"]))\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.bar(\n",
        "        x_pos,\n",
        "        overall_stats[\"target_mean\"],\n",
        "        yerr=overall_stats[\"target_std\"].fillna(0),\n",
        "        capsize=4,\n",
        "        color=\"#5DADE2\",\n",
        "    )\n",
        "    ax.set_ylabel(\"Target accuracy (%)\")\n",
        "    ax.set_title(\"Office-Home average across 12 transfers\")\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.set_xticks(x_pos)\n",
        "    ax.set_xticklabels(overall_stats[\"method\"], rotation=25, ha=\"right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Heatmap: transfer vs methods\n",
        "    pivot = transfer_stats.pivot(index=\"transfer\", columns=\"method\", values=\"target_mean\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    im = ax.imshow(pivot.values, cmap=\"viridis\")\n",
        "    ax.set_xticks(np.arange(len(pivot.columns)))\n",
        "    ax.set_xticklabels(pivot.columns, rotation=30, ha=\"right\")\n",
        "    ax.set_yticks(np.arange(len(pivot.index)))\n",
        "    ax.set_yticklabels(pivot.index)\n",
        "    ax.set_title(\"Target accuracy by transfer/method\")\n",
        "    cbar = plt.colorbar(im, ax=ax)\n",
        "    cbar.set_label(\"Target accuracy (%)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Optional: per-transfer comparison vs source_only\n",
        "    baseline = pivot[\"source_only\"] if \"source_only\" in pivot.columns else None\n",
        "    if baseline is not None:\n",
        "        x_idx = np.arange(len(pivot.index))\n",
        "        fig, ax = plt.subplots(figsize=(12, 4))\n",
        "        ax.plot(x_idx, baseline.values, label=\"source_only\", linestyle=\"--\", color=\"black\")\n",
        "        for method in METHODS:\n",
        "            if method == \"source_only\" or method not in pivot.columns:\n",
        "                continue\n",
        "            ax.plot(x_idx, pivot[method].values, label=method)\n",
        "        ax.set_ylabel(\"Target accuracy (%)\")\n",
        "        ax.set_xticks(x_idx)\n",
        "        ax.set_xticklabels(pivot.index, rotation=30, ha=\"right\")\n",
        "        ax.set_title(\"Per-transfer accuracy vs source_only\")\n",
        "        ax.legend(ncol=3)\n",
        "        plt.show()\n"
      ],
      "id": "JEWUdCMehgEP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTTvKwGxhgEQ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if globals().get('RUN_PIPELINE', 'camelyon17') != 'officehome':\n",
        "    print('Skipping legacy Office-Home section (set RUN_PIPELINE=\"officehome\" to run).')\n",
        "else:\n",
        "    import pandas as pd\n",
        "\n",
        "    status_counts = pd.Series([str(r.get(\"status\", \"unknown\")) for r in results]).value_counts()\n",
        "    succeeded = sum(1 for r in results if str(r.get(\"status\", \"\")).lower() in {\"done\", \"trained\", \"resumed\", \"skipped\"})\n",
        "    failed_runs = [r for r in results if str(r.get(\"status\", \"\")).startswith(\"fail\")]\n",
        "\n",
        "    print(f\"Expected runs: {expected_runs}\")\n",
        "    print(f\"Succeeded/skipped: {succeeded} | Failed: {len(failed_runs)}\")\n",
        "    print(\"Status counts:\")\n",
        "    print(status_counts)\n",
        "\n",
        "    if failed_runs:\n",
        "        print(\"Failed runs (see stderr in Drive if available):\")\n",
        "        for fr in failed_runs:\n",
        "            print(\n",
        "                f\" - {fr.get('method')} {fr.get('src')}->{fr.get('tgt')} seed={fr.get('seed')} \"\n",
        "                f\"err={fr.get('stderr', fr.get('error', ''))}\"\n",
        "            )\n",
        "    else:\n",
        "        print(\"No failed runs recorded.\")\n",
        "\n",
        "    print(f\"Summary CSV stored at: {SUMMARY_DRIVE_PATH}\")\n",
        "    print(f\"Run artifacts stored under: {RUNS_DRIVE / RUN_TAG}\")\n",
        "\n"
      ],
      "id": "wTTvKwGxhgEQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}