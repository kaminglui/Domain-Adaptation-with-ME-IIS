{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr6qDvk0hgD_"
      },
      "source": [
        "# ME-IIS Colab Benchmark (12 Office-Home transfers)\n",
        "\n",
        "Colab-first pipeline that caches datasets/checkpoints in Google Drive while training on local SSD.\n",
        "Pulls the latest `main`, sets up dependencies once, and runs the baseline UDA suite: `source_only`, `dann`, `dan`, `jan`, `cdan`, `me_iis`.\n",
        "Handles dataset bootstrap via KaggleHub, deterministic naming, and skips work when artifacts already exist.\n",
        "Executes all 12 Office-Home transfer pairs with seeds/config defined in one place.\n",
        "Uses GPU + AMP when available, auto-tunes dataloaders, and avoids Drive I/O during training.\n",
        "Aggregates metrics, plots, and writes summary CSVs + checkpoints back to Drive.\n",
        "        \n"
      ],
      "id": "Nr6qDvk0hgD_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYQs0NEphgEE",
        "outputId": "25f1e0c0-fc4d-4aad-aec2-d23137a51c0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Local /content disk: total=235.68GB free=197.21GB path=/content\n",
            "Drive disk: total=15.00GB free=5.92GB path=/content/drive/MyDrive/ME-IIS\n",
            "GPU: NVIDIA A100-SXM4-40GB free=39.14GB total=39.56GB\n",
            "CPU cores: 12\n",
            "RAM: total=83.47GB available=81.01GB\n",
            "Local disk snapshot: total=235.68GB free=197.21GB path=/content\n",
            "DRIVE_ROOT=/content/drive/MyDrive/ME-IIS\n",
            "Avoid saving checkpoints directly to Drive during training; final artifacts are copied after runs.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "else:\n",
        "    print(\"Not running inside Colab; skipping Drive mount (paths still target /content).\")\n",
        "\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/ME-IIS\")\n",
        "DATASETS_DRIVE = DRIVE_ROOT / \"datasets\"\n",
        "RUNS_DRIVE = DRIVE_ROOT / \"runs\"\n",
        "WORKDIR = Path(\"/content/work\")\n",
        "\n",
        "for path in [DRIVE_ROOT, DATASETS_DRIVE, RUNS_DRIVE, WORKDIR]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def _format_bytes(num):\n",
        "    try:\n",
        "        if num is None:\n",
        "            return \"unknown\"\n",
        "        num = float(num)\n",
        "        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n",
        "            if num < 1024 or unit == \"PB\":\n",
        "                return f\"{num:.2f}{unit}\"\n",
        "            num /= 1024.0\n",
        "    except Exception:\n",
        "        return str(num)\n",
        "\n",
        "\n",
        "def _print_disk(path, label):\n",
        "    try:\n",
        "        total, used, free = shutil.disk_usage(str(path))\n",
        "        print(f\"{label}: total={_format_bytes(total)} free={_format_bytes(free)} path={path}\")\n",
        "    except Exception as exc:\n",
        "        print(f\"{label}: unable to read disk usage ({exc})\")\n",
        "\n",
        "\n",
        "_print_disk(\"/content\", \"Local /content disk\")\n",
        "if DRIVE_ROOT.exists():\n",
        "    _print_disk(DRIVE_ROOT, \"Drive disk\")\n",
        "else:\n",
        "    print(\"Drive path missing (mount may have been skipped).\")\n",
        "\n",
        "\n",
        "def print_system_info():\n",
        "    gpu_msg = \"GPU: none detected\"\n",
        "    try:\n",
        "        import torch\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            props = torch.cuda.get_device_properties(0)\n",
        "            free, total = torch.cuda.mem_get_info()\n",
        "            gpu_msg = f\"GPU: {props.name} free={_format_bytes(free)} total={_format_bytes(total)}\"\n",
        "        else:\n",
        "            gpu_msg = \"GPU: CPU-only runtime\"\n",
        "    except Exception as exc:\n",
        "        gpu_msg = f\"GPU check failed: {exc}\"\n",
        "    print(gpu_msg)\n",
        "\n",
        "    cpu = os.cpu_count()\n",
        "    print(f\"CPU cores: {cpu}\")\n",
        "    try:\n",
        "        import psutil  # type: ignore\n",
        "\n",
        "        vm = psutil.virtual_memory()\n",
        "        print(f\"RAM: total={_format_bytes(vm.total)} available={_format_bytes(vm.available)}\")\n",
        "    except Exception:\n",
        "        print(\"RAM: psutil not available\")\n",
        "    _print_disk(\"/content\", \"Local disk snapshot\")\n",
        "\n",
        "\n",
        "print_system_info()\n",
        "print(f\"DRIVE_ROOT={DRIVE_ROOT}\")\n",
        "print(\"Avoid saving checkpoints directly to Drive during training; final artifacts are copied after runs.\")\n",
        "\n"
      ],
      "id": "vYQs0NEphgEE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqUyqSOKhgEH",
        "outputId": "54885134-e913-47d0-95ec-77f78e4fa87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repo...\n",
            "HEAD commit: b63ed8c326a6dee411cb4284965cc085b3b8ecf5\n",
            "Git status:\n",
            "## main...origin/main\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/kaminglui/ME-IIS.git\"\n",
        "REPO_DIR = Path(\"/content/ME-IIS\")\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    print(\"Cloning repo...\")\n",
        "    subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)])\n",
        "else:\n",
        "    print(\"Repo already present, fetching latest main...\")\n",
        "    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"fetch\", \"--all\", \"--tags\"])\n",
        "    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"checkout\", \"main\"])\n",
        "    subprocess.check_call([\"git\", \"-C\", str(REPO_DIR), \"pull\", \"--ff-only\"])\n",
        "\n",
        "sha = (\n",
        "    subprocess.check_output([\"git\", \"-C\", str(REPO_DIR), \"rev-parse\", \"HEAD\"], text=True)\n",
        "    .strip()\n",
        ")\n",
        "status = subprocess.check_output([\"git\", \"-C\", str(REPO_DIR), \"status\", \"-sb\"], text=True).strip()\n",
        "\n",
        "print(f\"HEAD commit: {sha}\")\n",
        "print(\"Git status:\")\n",
        "print(status)\n",
        "\n",
        "import sys\n",
        "\n",
        "if str(REPO_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_DIR))\n",
        "\n"
      ],
      "id": "EqUyqSOKhgEH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZb-lNnhgEI",
        "outputId": "85adcb69-8b98-4301-cd01-5b0581dd9f20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core packages already installed; skipping requirements install.\n",
            "kagglehub already installed.\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "REQ_FILE = Path(REPO_DIR) / \"requirements.txt\"\n",
        "\n",
        "\n",
        "def _is_installed(mod_name: str) -> bool:\n",
        "    try:\n",
        "        importlib.import_module(mod_name)\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "missing_core = [m for m in [\"torch\", \"torchvision\", \"sklearn\"] if not _is_installed(m)]\n",
        "if missing_core:\n",
        "    print(f\"Installing requirements (missing: {missing_core})...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(REQ_FILE)])\n",
        "else:\n",
        "    print(\"Core packages already installed; skipping requirements install.\")\n",
        "\n",
        "if not _is_installed(\"kagglehub\"):\n",
        "    print(\"Installing kagglehub for dataset downloads...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kagglehub\"])\n",
        "else:\n",
        "    print(\"kagglehub already installed.\")\n",
        "\n"
      ],
      "id": "rMZb-lNnhgEI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQVoku1hgEI",
        "outputId": "53d56491-1f54-4405-aa92-310cd895faf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset helpers ready. Use ensure_dataset_in_drive(...) then maybe_copy_dataset_to_local(...).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "OFFICE_HOME_DOMAIN_SETS = [\n",
        "    [\"Art\", \"Clipart\", \"Product\", \"RealWorld\"],\n",
        "    [\"Art\", \"Clipart\", \"Product\", \"Real World\"],\n",
        "    [\"Art\", \"Clipart\", \"Product\", \"Real_World\"],\n",
        "    [\"Art\", \"Clipart\", \"Product\", \"Real\"],\n",
        "]\n",
        "OFFICE31_DOMAINS = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "\n",
        "\n",
        "def _maybe_extract_zip(path: Path) -> Path:\n",
        "    if path.is_file() and path.suffix.lower() == \".zip\":\n",
        "        target = path.with_suffix(\"\")\n",
        "        target.mkdir(parents=True, exist_ok=True)\n",
        "        marker = target / \".extracted_ok\"\n",
        "        if not marker.exists():\n",
        "            print(f\"[DATA] Extracting {path} -> {target}\")\n",
        "            with zipfile.ZipFile(path, \"r\") as zf:\n",
        "                zf.extractall(target)\n",
        "            marker.write_text(\"ok\", encoding=\"utf-8\")\n",
        "        return target\n",
        "    return path\n",
        "\n",
        "\n",
        "def _find_office_home_root(raw: Path) -> Path:\n",
        "    raw = _maybe_extract_zip(Path(raw))\n",
        "    candidates = [raw] + [p for p in raw.rglob(\"*\") if p.is_dir()]\n",
        "    for candidate in candidates:\n",
        "        for domains in OFFICE_HOME_DOMAIN_SETS:\n",
        "            if all((candidate / d).exists() for d in domains):\n",
        "                return candidate\n",
        "    raise FileNotFoundError(f\"Could not locate Office-Home domains under {raw}\")\n",
        "\n",
        "\n",
        "def _find_office31_root(raw: Path) -> Path:\n",
        "    raw = _maybe_extract_zip(Path(raw))\n",
        "    candidates = [raw] + [p for p in raw.rglob(\"*\") if p.is_dir()]\n",
        "    for candidate in candidates:\n",
        "        if all((candidate / d).exists() for d in OFFICE31_DOMAINS):\n",
        "            return candidate\n",
        "    raise FileNotFoundError(f\"Could not locate Office-31 domains under {raw}\")\n",
        "\n",
        "\n",
        "def resolve_dataset_root(dataset_name: str) -> Path:\n",
        "    if dataset_name == \"office_home\":\n",
        "        raw = Path(kagglehub.dataset_download(\"lhrrraname/officehome\"))\n",
        "        return _find_office_home_root(raw)\n",
        "    if dataset_name == \"office31\":\n",
        "        raw = Path(kagglehub.dataset_download(\"xixuhu/office31\"))\n",
        "        return _find_office31_root(raw)\n",
        "    raise ValueError(f\"Unknown DATASET_NAME={dataset_name}\")\n",
        "\n",
        "\n",
        "def _dir_size_bytes(path: Path) -> int:\n",
        "    total = 0\n",
        "    for fp in path.rglob(\"*\"):\n",
        "        if fp.is_file():\n",
        "            try:\n",
        "                total += int(fp.stat().st_size)\n",
        "            except Exception:\n",
        "                continue\n",
        "    return total\n",
        "\n",
        "\n",
        "def ensure_dataset_in_drive(dataset_name: str, datasets_drive_dir: Path) -> Path:\n",
        "    datasets_drive_dir = Path(datasets_drive_dir)\n",
        "    datasets_drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "    drive_path = datasets_drive_dir / dataset_name\n",
        "    if drive_path.exists():\n",
        "        print(f\"[DATA] Using cached dataset in Drive: {drive_path}\")\n",
        "        return drive_path\n",
        "    raw_root = resolve_dataset_root(dataset_name)\n",
        "    drive_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[DATA] Copying dataset into Drive cache: {raw_root} -> {drive_path}\")\n",
        "    shutil.copytree(raw_root, drive_path, dirs_exist_ok=True)\n",
        "    return drive_path\n",
        "\n",
        "\n",
        "def maybe_copy_dataset_to_local(drive_path: Path, local_path: Path, enable: bool = True, headroom_gb: int = 5) -> Path:\n",
        "    drive_path = Path(drive_path)\n",
        "    local_path = Path(local_path)\n",
        "    if not enable:\n",
        "        print(f\"[DATA] Local caching disabled; using Drive path: {drive_path}\")\n",
        "        return drive_path\n",
        "    marker = local_path / \".cached_ok\"\n",
        "    if marker.exists():\n",
        "        print(f\"[DATA] Using existing local copy: {local_path}\")\n",
        "        return local_path\n",
        "    try:\n",
        "        free = shutil.disk_usage(str(local_path.parent)).free\n",
        "    except Exception:\n",
        "        free = None\n",
        "    dataset_size = _dir_size_bytes(drive_path)\n",
        "    if free is not None and dataset_size > max(0, free - headroom_gb * 1024**3):\n",
        "        print(f\"[DATA] Insufficient local disk; using Drive path ({drive_path})\")\n",
        "        return drive_path\n",
        "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[DATA] Copying dataset from Drive to local SSD: {drive_path} -> {local_path}\")\n",
        "    shutil.copytree(drive_path, local_path, dirs_exist_ok=True)\n",
        "    marker.write_text(\"ok\", encoding=\"utf-8\")\n",
        "    return local_path\n",
        "\n",
        "\n",
        "print(\"Dataset helpers ready. Use ensure_dataset_in_drive(...) then maybe_copy_dataset_to_local(...).\")\n",
        "\n"
      ],
      "id": "bYQVoku1hgEI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXIhx8OThgEK",
        "outputId": "8893d8c3-b740-456c-a75d-2a16a03341cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config ready | RUN_TAG=office_home_full_20251216_050010 | METHODS=['source_only', 'dann', 'dan', 'jan', 'cdan', 'me_iis'] | DOMAINS=['Ar', 'Cl', 'Pr', 'Rw']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4137037307.py:27: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "DATASETS_LOCAL = WORKDIR / \"datasets\"\n",
        "RUNS_LOCAL = WORKDIR / \"runs\"\n",
        "RUNS_LOCAL_INTERNAL = WORKDIR / \"runs_internal\"\n",
        "for path in [DATASETS_LOCAL, RUNS_LOCAL, RUNS_LOCAL_INTERNAL]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATASET_NAME = \"office_home\"\n",
        "DOMAINS = [\"Ar\", \"Cl\", \"Pr\", \"Rw\"] if DATASET_NAME == \"office_home\" else [\"A\", \"D\", \"W\"]\n",
        "METHODS = [\"source_only\", \"dann\", \"dan\", \"jan\", \"cdan\", \"me_iis\"]  # CORAL intentionally excluded\n",
        "SEEDS = [0]  # for full benchmark: [0, 1, 2]\n",
        "\n",
        "EPOCHS_SOURCE = 50\n",
        "EPOCHS_ADAPT = 10\n",
        "BATCH_SIZE = 32  # set to \"auto\" to probe largest fitting batch\n",
        "NUM_WORKERS = \"auto\"  # auto-tuned below\n",
        "AMP = True\n",
        "DETERMINISTIC = False\n",
        "FORCE_RERUN = False\n",
        "SAVE_EVERY_EPOCH = False\n",
        "\n",
        "BACKBONE = \"resnet50\"\n",
        "INPUT_SIZE = 224\n",
        "\n",
        "timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RUN_TAG = f\"{DATASET_NAME}_full_{timestamp}\"\n",
        "\n",
        "SOURCE_ONLY_PARAMS = {\"amp\": AMP}\n",
        "\n",
        "DANN_PARAMS = {\n",
        "    \"amp\": AMP,\n",
        "    \"disc_hidden_dim\": 1024,\n",
        "    \"disc_dropout\": 0.1,\n",
        "    \"grl_lambda_schedule\": \"grl\",\n",
        "    \"domain_loss_weight\": 1.0,\n",
        "}\n",
        "\n",
        "DAN_PARAMS = {\n",
        "    \"amp\": AMP,\n",
        "    \"mmd_loss_weight\": 1.0,\n",
        "    \"mmd_kernel_mul\": 2.0,\n",
        "    \"mmd_kernel_num\": 5,\n",
        "    \"mmd_fix_sigma\": None,\n",
        "}\n",
        "\n",
        "JAN_PARAMS = {\n",
        "    \"amp\": AMP,\n",
        "    \"jmmd_loss_weight\": 1.0,\n",
        "    \"jmmd_kernel_mul\": 2.0,\n",
        "    \"jmmd_kernel_num\": 5,\n",
        "    \"jmmd_fix_sigma\": None,\n",
        "}\n",
        "\n",
        "CDAN_PARAMS = {\n",
        "    \"amp\": AMP,\n",
        "    \"disc_hidden_dim\": 1024,\n",
        "    \"disc_dropout\": 0.1,\n",
        "    \"domain_loss_weight\": 1.0,\n",
        "    \"grl_lambda_schedule\": \"grl\",\n",
        "    \"entropy_conditioning\": False,\n",
        "}\n",
        "\n",
        "ME_IIS_PARAMS = {\n",
        "    \"amp\": AMP,\n",
        "    \"feature_layers\": [\"layer3\", \"layer4\"],\n",
        "    \"num_latent_styles\": 5,\n",
        "    \"gmm_bic_min_components\": 2,\n",
        "    \"gmm_bic_max_components\": 8,\n",
        "    \"gmm_reg_covar\": 1e-6,\n",
        "    \"gmm_selection_mode\": \"bic\",\n",
        "    \"iis_iters\": 15,\n",
        "    \"iis_tol\": 1e-3,\n",
        "    \"cluster_backend\": \"gmm\",\n",
        "    \"cluster_clean_ratio\": 1.0,\n",
        "    \"kmeans_n_init\": 10,\n",
        "    \"source_prob_mode\": \"softmax\",\n",
        "    \"use_pseudo_labels\": False,\n",
        "    \"pseudo_conf_thresh\": 0.9,\n",
        "    \"pseudo_max_ratio\": 1.0,\n",
        "    \"pseudo_loss_weight\": 1.0,\n",
        "    \"weight_clip_max\": None,\n",
        "    \"weight_mix_alpha\": 0.0,\n",
        "}\n",
        "\n",
        "METHOD_PARAMS = {\n",
        "    \"source_only\": SOURCE_ONLY_PARAMS,\n",
        "    \"dann\": DANN_PARAMS,\n",
        "    \"dan\": DAN_PARAMS,\n",
        "    \"jan\": JAN_PARAMS,\n",
        "    \"cdan\": CDAN_PARAMS,\n",
        "    \"me_iis\": ME_IIS_PARAMS,\n",
        "}\n",
        "\n",
        "os.environ.setdefault(\"ME_IIS_AUTO_RESOURCES\", \"1\")\n",
        "print(f\"Config ready | RUN_TAG={RUN_TAG} | METHODS={METHODS} | DOMAINS={DOMAINS}\")\n",
        "\n"
      ],
      "id": "lXIhx8OThgEK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE-6flSwhgEL"
      },
      "execution_count": 7,
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "\n",
        "def _layer_tag(params: Dict[str, Any]) -> str:\n",
        "    layers = params.get(\"feature_layers\", [\"layer3\", \"layer4\"])\n",
        "    if isinstance(layers, str):\n",
        "        layers = [p.strip() for p in layers.split(\",\") if p.strip()]\n",
        "    return \"-\".join(layers)\n",
        "\n",
        "\n",
        "def make_run_dir(method: str, dataset: str, src: str, tgt: str, seed: int, base: Path | None = None) -> Path:\n",
        "    base = Path(base or RUNS_LOCAL)\n",
        "    return base / str(RUN_TAG) / str(dataset) / f\"{src}2{tgt}\" / str(method) / f\"seed{seed}\"\n",
        "\n",
        "\n",
        "def make_ckpt_name(\n",
        "    method: str,\n",
        "    dataset: str,\n",
        "    src: str,\n",
        "    tgt: str,\n",
        "    seed: int,\n",
        "    *,\n",
        "    batch_size: Any | None = None,\n",
        "    epochs_source: Any | None = None,\n",
        "    epochs_adapt: Any | None = None,\n",
        "    method_params: Dict[str, Any] | None = None,\n",
        ") -> str:\n",
        "    bs_tag = batch_size if batch_size is not None else BATCH_SIZE\n",
        "    es_tag = epochs_source if epochs_source is not None else EPOCHS_SOURCE\n",
        "    ea_tag = epochs_adapt if epochs_adapt is not None else EPOCHS_ADAPT\n",
        "    name = f\"{method}__{dataset}__{src}2{tgt}__seed{seed}__Esrc{es_tag}__Ead{ea_tag}__bs{bs_tag}\"\n",
        "    if method == \"me_iis\":\n",
        "        params = method_params or METHOD_PARAMS.get(\"me_iis\", {})\n",
        "        layer_tag = _layer_tag(params)\n",
        "        gmm_min = params.get(\"gmm_bic_min_components\", 2)\n",
        "        gmm_max = params.get(\"gmm_bic_max_components\", 8)\n",
        "        iis_iters = params.get(\"iis_iters\", 15)\n",
        "        iis_tol = params.get(\"iis_tol\", 1e-3)\n",
        "        name += f\"__layers{layer_tag}__gmm{gmm_min}-{gmm_max}__iis{iis_iters}-{iis_tol}\"\n",
        "    return name + \".pt\"\n",
        "\n",
        "\n",
        "def decode_ckpt_name(name: str) -> Dict[str, Any]:\n",
        "    stem = Path(name).name.replace(\".pt\", \"\")\n",
        "    parts = stem.split(\"__\")\n",
        "    info: Dict[str, Any] = {\"raw\": stem}\n",
        "    try:\n",
        "        method, dataset, pair, seed_part, esrc_part, ead_part, bs_part, *rest = parts\n",
        "        src, tgt = pair.split(\"2\", 1)\n",
        "        info.update(\n",
        "            {\n",
        "                \"method\": method,\n",
        "                \"dataset\": dataset,\n",
        "                \"src\": src,\n",
        "                \"tgt\": tgt,\n",
        "                \"seed\": int(seed_part.replace(\"seed\", \"\")),\n",
        "                \"epochs_source\": esrc_part.replace(\"Esrc\", \"\"),\n",
        "                \"epochs_adapt\": ead_part.replace(\"Ead\", \"\"),\n",
        "                \"batch_size\": bs_part.replace(\"bs\", \"\"),\n",
        "            }\n",
        "        )\n",
        "        for token in rest:\n",
        "            if token.startswith(\"layers\"):\n",
        "                info[\"layers\"] = token.replace(\"layers\", \"\")\n",
        "            elif token.startswith(\"gmm\"):\n",
        "                rng = token.replace(\"gmm\", \"\")\n",
        "                if \"-\" in rng:\n",
        "                    lo, hi = rng.split(\"-\", 1)\n",
        "                    info[\"gmm_range\"] = (int(lo), int(hi))\n",
        "            elif token.startswith(\"iis\"):\n",
        "                payload = token.replace(\"iis\", \"\")\n",
        "                if \"-\" in payload:\n",
        "                    iters, tol = payload.split(\"-\", 1)\n",
        "                    info[\"iis_iters\"] = int(float(iters))\n",
        "                    info[\"iis_tol\"] = float(tol)\n",
        "    except Exception:\n",
        "        info[\"error\"] = \"unable to decode\"\n",
        "    return info\n",
        "\n"
      ],
      "id": "lE-6flSwhgEL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uovVkWxhgEL",
        "outputId": "65d2263d-214b-40b7-fc29-f592dcd3ca22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto dataloader params: {'num_workers': 8, 'pin_memory': True, 'prefetch_factor': 2, 'persistent_workers': True}\n",
            "Batch size (requested=32) resolved to: 32\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def set_global_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = bool(DETERMINISTIC)\n",
        "    torch.backends.cudnn.benchmark = not bool(DETERMINISTIC)\n",
        "\n",
        "\n",
        "def auto_dataloader_params(requested: Any = NUM_WORKERS) -> Dict[str, Any]:\n",
        "    cpu = os.cpu_count() or 2\n",
        "    num_workers = requested\n",
        "    if requested == \"auto\":\n",
        "        num_workers = min(8, max(2, cpu - 1))\n",
        "        try:\n",
        "            import psutil  # type: ignore\n",
        "\n",
        "            available = psutil.virtual_memory().available\n",
        "            if available < 8 * 1024**3:\n",
        "                num_workers = max(0, min(num_workers, 2))\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        num_workers = int(num_workers)\n",
        "    except Exception:\n",
        "        num_workers = 0\n",
        "    if num_workers < 0:\n",
        "        num_workers = 0\n",
        "    pin_memory = torch.cuda.is_available()\n",
        "    prefetch = 2 if num_workers > 0 else None\n",
        "    persistent_workers = bool(num_workers > 0)\n",
        "    return {\n",
        "        \"num_workers\": num_workers,\n",
        "        \"pin_memory\": pin_memory,\n",
        "        \"prefetch_factor\": prefetch,\n",
        "        \"persistent_workers\": persistent_workers,\n",
        "    }\n",
        "\n",
        "\n",
        "def auto_batch_size(candidates=(64, 32, 16), device: str | None = None) -> int:\n",
        "    if BATCH_SIZE != \"auto\":\n",
        "        return int(BATCH_SIZE)\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for bs in candidates:\n",
        "        try:\n",
        "            _ = torch.zeros((bs, 3, INPUT_SIZE, INPUT_SIZE), device=device)\n",
        "            del _\n",
        "            return int(bs)\n",
        "        except RuntimeError as exc:\n",
        "            if \"out of memory\" in str(exc).lower():\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                continue\n",
        "            raise\n",
        "    return int(candidates[-1])\n",
        "\n",
        "\n",
        "AUTO_DL = auto_dataloader_params(NUM_WORKERS)\n",
        "AUTO_BATCH = auto_batch_size()\n",
        "print(f\"Auto dataloader params: {AUTO_DL}\")\n",
        "print(f\"Batch size (requested={BATCH_SIZE}) resolved to: {AUTO_BATCH}\")\n",
        "\n"
      ],
      "id": "8uovVkWxhgEL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NSlUjCnhgEM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "from src.experiments.run_config import RunConfig, get_run_dir\n",
        "from src.experiments.runner import run_one\n",
        "\n",
        "\n",
        "def _read_metrics_csv(path: Path) -> tuple[float | None, float | None, Dict[str, Any] | None]:\n",
        "    if not path.exists():\n",
        "        return None, None, None\n",
        "    try:\n",
        "        with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            rows = list(reader)\n",
        "    except Exception:\n",
        "        return None, None, None\n",
        "    if not rows:\n",
        "        return None, None, None\n",
        "    row = rows[-1]\n",
        "    try:\n",
        "        src_acc = float(row.get(\"source_acc\", \"nan\"))\n",
        "    except Exception:\n",
        "        src_acc = None\n",
        "    try:\n",
        "        tgt_acc = float(row.get(\"target_acc\", \"nan\"))\n",
        "    except Exception:\n",
        "        tgt_acc = None\n",
        "    return src_acc, tgt_acc, row\n",
        "\n",
        "\n",
        "def _safe_copy(src: Path, dst: Path) -> None:\n",
        "    if src.exists():\n",
        "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "\n",
        "def run_experiment(\n",
        "    method: str,\n",
        "    src: str,\n",
        "    tgt: str,\n",
        "    seed: int,\n",
        "    data_root: Path,\n",
        "    runs_dir_local: Path,\n",
        "    runs_dir_drive: Path,\n",
        ") -> Dict[str, Any]:\n",
        "    method_params = dict(METHOD_PARAMS.get(method, {}))\n",
        "    method_params.setdefault(\"amp\", AMP)\n",
        "    resolved_batch_size = auto_batch_size()\n",
        "    dl_params = auto_dataloader_params(NUM_WORKERS)\n",
        "    resolved_num_workers = dl_params[\"num_workers\"]\n",
        "\n",
        "    run_dir_local = make_run_dir(method, DATASET_NAME, src, tgt, seed, base=runs_dir_local)\n",
        "    run_dir_drive = make_run_dir(method, DATASET_NAME, src, tgt, seed, base=runs_dir_drive)\n",
        "    run_dir_local.mkdir(parents=True, exist_ok=True)\n",
        "    run_dir_drive.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ckpt_name = make_ckpt_name(\n",
        "        method,\n",
        "        DATASET_NAME,\n",
        "        src,\n",
        "        tgt,\n",
        "        seed,\n",
        "        batch_size=resolved_batch_size,\n",
        "        method_params=method_params,\n",
        "    )\n",
        "    metrics_drive_path = run_dir_drive / \"metrics.csv\"\n",
        "    ckpt_drive_path = run_dir_drive / ckpt_name\n",
        "\n",
        "    config_payload = {\n",
        "        \"run_tag\": RUN_TAG,\n",
        "        \"dataset\": DATASET_NAME,\n",
        "        \"src\": src,\n",
        "        \"tgt\": tgt,\n",
        "        \"seed\": seed,\n",
        "        \"method\": method,\n",
        "        \"epochs_source\": EPOCHS_SOURCE,\n",
        "        \"epochs_adapt\": EPOCHS_ADAPT,\n",
        "        \"batch_size\": resolved_batch_size,\n",
        "        \"num_workers\": resolved_num_workers,\n",
        "        \"method_params\": method_params,\n",
        "        \"amp\": AMP,\n",
        "        \"deterministic\": DETERMINISTIC,\n",
        "        \"data_root\": str(data_root),\n",
        "        \"run_dir_local\": str(run_dir_local),\n",
        "        \"run_dir_drive\": str(run_dir_drive),\n",
        "    }\n",
        "    for dest in [run_dir_local, run_dir_drive]:\n",
        "        (Path(dest) / \"config.json\").write_text(json.dumps(config_payload, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    if metrics_drive_path.exists() and ckpt_drive_path.exists() and not FORCE_RERUN:\n",
        "        src_acc, tgt_acc, row = _read_metrics_csv(metrics_drive_path)\n",
        "        print(f\"[SKIP] found existing artifacts for {method} {src}->{tgt} seed={seed}\")\n",
        "        return {\n",
        "            \"status\": \"skipped\",\n",
        "            \"method\": method,\n",
        "            \"src\": src,\n",
        "            \"tgt\": tgt,\n",
        "            \"seed\": seed,\n",
        "            \"source_acc\": src_acc,\n",
        "            \"target_acc\": tgt_acc,\n",
        "            \"metrics_csv\": str(metrics_drive_path),\n",
        "            \"checkpoint\": str(ckpt_drive_path),\n",
        "            \"run_dir\": str(run_dir_drive),\n",
        "            \"config\": config_payload,\n",
        "        }\n",
        "\n",
        "    print(f\"[RUN] method={method} src={src} tgt={tgt} seed={seed}\")\n",
        "    cfg = RunConfig(\n",
        "        dataset_name=DATASET_NAME,\n",
        "        data_root=str(data_root),\n",
        "        source_domain=src,\n",
        "        target_domain=tgt,\n",
        "        method=method,\n",
        "        backbone=BACKBONE,\n",
        "        input_size=INPUT_SIZE,\n",
        "        epochs_source=EPOCHS_SOURCE,\n",
        "        epochs_adapt=EPOCHS_ADAPT,\n",
        "        batch_size=int(resolved_batch_size),\n",
        "        num_workers=int(resolved_num_workers),\n",
        "        method_params=method_params,\n",
        "        seed=int(seed),\n",
        "        deterministic=bool(DETERMINISTIC),\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = run_one(\n",
        "            cfg,\n",
        "            force_rerun=FORCE_RERUN,\n",
        "            runs_root=RUNS_LOCAL_INTERNAL,\n",
        "            write_metrics=True,\n",
        "            raise_on_error=False,\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        err = traceback.format_exc()\n",
        "        err_path = run_dir_drive / \"stderr.txt\"\n",
        "        err_path.write_text(err, encoding=\"utf-8\")\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"method\": method,\n",
        "            \"src\": src,\n",
        "            \"tgt\": tgt,\n",
        "            \"seed\": seed,\n",
        "            \"error\": str(exc),\n",
        "            \"stderr\": str(err_path),\n",
        "            \"run_dir\": str(run_dir_drive),\n",
        "        }\n",
        "\n",
        "    internal_run_dir = get_run_dir(cfg, runs_root=RUNS_LOCAL_INTERNAL)\n",
        "    metrics_internal = internal_run_dir / \"metrics.csv\"\n",
        "    logs_internal = internal_run_dir / \"logs\"\n",
        "    ckpt_internal = None\n",
        "    if isinstance(result, dict) and \"checkpoint\" in result:\n",
        "        ckpt_internal = Path(result.get(\"checkpoint\")) if result.get(\"checkpoint\") else None\n",
        "    if ckpt_internal is None or not ckpt_internal.exists():\n",
        "        for cand in (internal_run_dir / \"checkpoints\").glob(\"*final*.pth\"):\n",
        "            ckpt_internal = cand\n",
        "            break\n",
        "\n",
        "    _safe_copy(metrics_internal, run_dir_local / \"metrics.csv\")\n",
        "    _safe_copy(metrics_internal, metrics_drive_path)\n",
        "    if logs_internal.exists():\n",
        "        shutil.copytree(logs_internal, run_dir_local / \"logs\", dirs_exist_ok=True)\n",
        "        shutil.copytree(logs_internal, run_dir_drive / \"logs\", dirs_exist_ok=True)\n",
        "    if ckpt_internal is not None and ckpt_internal.exists():\n",
        "        _safe_copy(ckpt_internal, run_dir_local / ckpt_name)\n",
        "        _safe_copy(ckpt_internal, ckpt_drive_path)\n",
        "\n",
        "    src_acc, tgt_acc, row = _read_metrics_csv(run_dir_local / \"metrics.csv\")\n",
        "    status = result.get(\"status\", \"done\") if isinstance(result, dict) else \"done\"\n",
        "    print(f\"[DONE] method={method} src={src} tgt={tgt} seed={seed} target_acc={tgt_acc}\")\n",
        "    return {\n",
        "        \"status\": status,\n",
        "        \"method\": method,\n",
        "        \"src\": src,\n",
        "        \"tgt\": tgt,\n",
        "        \"seed\": seed,\n",
        "        \"source_acc\": src_acc,\n",
        "        \"target_acc\": tgt_acc,\n",
        "        \"metrics_csv\": str(run_dir_local / \"metrics.csv\"),\n",
        "        \"checkpoint\": str(ckpt_drive_path if ckpt_drive_path.exists() else ckpt_internal or \"\"),\n",
        "        \"run_dir\": str(run_dir_drive),\n",
        "        \"config\": config_payload,\n",
        "        \"logs\": str(run_dir_drive / \"logs\"),\n",
        "    }\n",
        "\n"
      ],
      "id": "2NSlUjCnhgEM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "U4JofiFEiOcZ"
      },
      "id": "U4JofiFEiOcZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dStORBkShgEO",
        "outputId": "067d56c7-f403-4f5d-e8f4-be660e496b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'officehome' dataset.\n",
            "[DATA] Copying dataset into Drive cache: /kaggle/input/officehome/datasets/OfficeHomeDataset_10072016 -> /content/drive/MyDrive/ME-IIS/datasets/office_home\n",
            "[DATA] Copying dataset from Drive to local SSD: /content/drive/MyDrive/ME-IIS/datasets/office_home -> /content/work/datasets/office_home\n",
            "[DATA] Training will read from: /content/work/datasets/office_home\n",
            "[RUN] method=source_only src=Ar tgt=Cl seed=0\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "transfer_pairs = [(s, t) for s in DOMAINS for t in DOMAINS if s != t]\n",
        "expected_runs = len(transfer_pairs) * len(SEEDS) * len(METHODS)\n",
        "\n",
        "# Dataset bootstrap: ensure Drive cache -> optional local SSD copy\n",
        "DATA_ROOT_DRIVE = ensure_dataset_in_drive(DATASET_NAME, DATASETS_DRIVE)\n",
        "DATA_ROOT_LOCAL = maybe_copy_dataset_to_local(DATA_ROOT_DRIVE, DATASETS_LOCAL / DATASET_NAME, enable=True)\n",
        "print(f\"[DATA] Training will read from: {DATA_ROOT_LOCAL}\")\n",
        "\n",
        "results: list[dict] = []\n",
        "for src, tgt in transfer_pairs:\n",
        "    for seed in SEEDS:\n",
        "        base_res = run_experiment(\"source_only\", src, tgt, seed, DATA_ROOT_LOCAL, RUNS_LOCAL, RUNS_DRIVE)\n",
        "        results.append(base_res)\n",
        "        for method in METHODS:\n",
        "            if method == \"source_only\":\n",
        "                continue\n",
        "            res = run_experiment(method, src, tgt, seed, DATA_ROOT_LOCAL, RUNS_LOCAL, RUNS_DRIVE)\n",
        "            results.append(res)\n",
        "\n",
        "print(f\"Finished benchmark loop: {len(results)} runs (expected {expected_runs}).\")\n",
        "\n"
      ],
      "id": "dStORBkShgEO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhupEFethgEP"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import shutil\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df[\"transfer\"] = results_df.apply(lambda r: f\"{r.get('src','?')}->{r.get('tgt','?')}\", axis=1)\n",
        "\n",
        "SUMMARY_LOCAL_PATH = WORKDIR / f\"summary_{RUN_TAG}.csv\"\n",
        "SUMMARY_DRIVE_PATH = RUNS_DRIVE / f\"summary_{RUN_TAG}.csv\"\n",
        "results_df.to_csv(SUMMARY_LOCAL_PATH, index=False)\n",
        "SUMMARY_DRIVE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copy2(SUMMARY_LOCAL_PATH, SUMMARY_DRIVE_PATH)\n",
        "print(f\"Saved summary CSV to {SUMMARY_LOCAL_PATH} and {SUMMARY_DRIVE_PATH}\")\n",
        "\n",
        "valid = results_df[results_df[\"target_acc\"].notnull()]\n",
        "transfer_stats = (\n",
        "    valid.groupby([\"method\", \"transfer\"])\n",
        "    .agg(target_mean=(\"target_acc\", \"mean\"), target_std=(\"target_acc\", \"std\"))\n",
        "    .reset_index()\n",
        ")\n",
        "overall_stats = (\n",
        "    valid.groupby(\"method\")\n",
        "    .agg(target_mean=(\"target_acc\", \"mean\"), target_std=(\"target_acc\", \"std\"))\n",
        "    .reset_index()\n",
        ")\n",
        "overall_stats[\"transfer\"] = \"Office-Home Avg\"\n",
        "\n",
        "print(\"Per-transfer stats (head):\")\n",
        "display(transfer_stats.head())\n",
        "print(\"Overall method means:\")\n",
        "display(overall_stats)\n",
        "\n"
      ],
      "id": "lhupEFethgEP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEWUdCMehgEP"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bar chart: average target accuracy per method\n",
        "x_pos = np.arange(len(overall_stats[\"method\"]))\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(\n",
        "    x_pos,\n",
        "    overall_stats[\"target_mean\"],\n",
        "    yerr=overall_stats[\"target_std\"].fillna(0),\n",
        "    capsize=4,\n",
        "    color=\"#5DADE2\",\n",
        ")\n",
        "ax.set_ylabel(\"Target accuracy (%)\")\n",
        "ax.set_title(\"Office-Home average across 12 transfers\")\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(overall_stats[\"method\"], rotation=25, ha=\"right\")\n",
        "plt.show()\n",
        "\n",
        "# Heatmap: transfer vs methods\n",
        "pivot = transfer_stats.pivot(index=\"transfer\", columns=\"method\", values=\"target_mean\")\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(pivot.values, cmap=\"viridis\")\n",
        "ax.set_xticks(np.arange(len(pivot.columns)))\n",
        "ax.set_xticklabels(pivot.columns, rotation=30, ha=\"right\")\n",
        "ax.set_yticks(np.arange(len(pivot.index)))\n",
        "ax.set_yticklabels(pivot.index)\n",
        "ax.set_title(\"Target accuracy by transfer/method\")\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "cbar.set_label(\"Target accuracy (%)\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: per-transfer comparison vs source_only\n",
        "baseline = pivot[\"source_only\"] if \"source_only\" in pivot.columns else None\n",
        "if baseline is not None:\n",
        "    x_idx = np.arange(len(pivot.index))\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.plot(x_idx, baseline.values, label=\"source_only\", linestyle=\"--\", color=\"black\")\n",
        "    for method in METHODS:\n",
        "        if method == \"source_only\" or method not in pivot.columns:\n",
        "            continue\n",
        "        ax.plot(x_idx, pivot[method].values, label=method)\n",
        "    ax.set_ylabel(\"Target accuracy (%)\")\n",
        "    ax.set_xticks(x_idx)\n",
        "    ax.set_xticklabels(pivot.index, rotation=30, ha=\"right\")\n",
        "    ax.set_title(\"Per-transfer accuracy vs source_only\")\n",
        "    ax.legend(ncol=3)\n",
        "    plt.show()\n"
      ],
      "id": "JEWUdCMehgEP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTTvKwGxhgEQ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "status_counts = pd.Series([str(r.get(\"status\", \"unknown\")) for r in results]).value_counts()\n",
        "succeeded = sum(1 for r in results if str(r.get(\"status\", \"\")).lower() in {\"done\", \"trained\", \"resumed\", \"skipped\"})\n",
        "failed_runs = [r for r in results if str(r.get(\"status\", \"\")).startswith(\"fail\")]\n",
        "\n",
        "print(f\"Expected runs: {expected_runs}\")\n",
        "print(f\"Succeeded/skipped: {succeeded} | Failed: {len(failed_runs)}\")\n",
        "print(\"Status counts:\")\n",
        "print(status_counts)\n",
        "\n",
        "if failed_runs:\n",
        "    print(\"Failed runs (see stderr in Drive if available):\")\n",
        "    for fr in failed_runs:\n",
        "        print(\n",
        "            f\" - {fr.get('method')} {fr.get('src')}->{fr.get('tgt')} seed={fr.get('seed')} \"\n",
        "            f\"err={fr.get('stderr', fr.get('error', ''))}\"\n",
        "        )\n",
        "else:\n",
        "    print(\"No failed runs recorded.\")\n",
        "\n",
        "print(f\"Summary CSV stored at: {SUMMARY_DRIVE_PATH}\")\n",
        "print(f\"Run artifacts stored under: {RUNS_DRIVE / RUN_TAG}\")\n",
        "\n"
      ],
      "id": "wTTvKwGxhgEQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}